{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "GLecDiHbogvX",
        "eo_QP1ITFfX2"
      ],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hublun/Beyond-ChatGPT/blob/main/The_Attention_Mechanism_in_GPT_2_AIM_Event.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised Pre-Training of GPT-Style Model\n",
        "\n",
        "In today's notebook, we'll be working through an example of how to do unsupervised pre-training of a GPT-style model.\n",
        "\n",
        "The base model we'll use is Andrej Karpathy's [nanoGPT](https://github.com/karpathy/nanoGPT).\n",
        "\n",
        "All of the model code can be found in the [`model.py`](https://github.com/karpathy/nanoGPT/blob/master/model.py) file!\n",
        "\n",
        "> NOTE: We will not be leveraging the parallized training strategy in this notebook - you can find all the required code in the provided repository."
      ],
      "metadata": {
        "id": "UWiGVj6njoDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Architecture Diagram\n",
        "\n",
        "This is the diagram we'll be using to guide our later intuitions about the model architecture!\n",
        "\n",
        "![image](https://i.imgur.com/WtcoFOo.png)"
      ],
      "metadata": {
        "id": "GBGw2a0bfT-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Selection\n",
        "\n",
        "For the notebook today, we'll be using a toy dataset called `tinyshakespeare`. Feel free to use your own corpus here, just make sure it's contained within a single `.txt` file.\n",
        "\n",
        "You could extend this example to use the [OpenWebText](https://skylion007.github.io/OpenWebTextCorpus/) dataset, which was used to pre-train GPT-2.\n",
        "\n",
        "> NOTE: Training LLMs can take a very long time - in order to get results similar to the [GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) you will need 8xA100s and train for ~4-5 days using a pararellized strategy (DDP) on the OpenWebText Corpus.\n",
        "\n",
        "Let's start by grabbing our source repository for the day!"
      ],
      "metadata": {
        "id": "eHi04aEnkKEZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMRsEQZy6tgc",
        "outputId": "f10fc37b-f55a-4827-8ece-84f9e3ee5aaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 649, done.\u001b[K\n",
            "remote: Total 649 (delta 0), reused 0 (delta 0), pack-reused 649\u001b[K\n",
            "Receiving objects: 100% (649/649), 936.45 KiB | 13.01 MiB/s, done.\n",
            "Resolving deltas: 100% (371/371), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/karpathy/nanoGPT.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll need to grab some dependencies.\n",
        "\n",
        "`cohere` and `openai` are recent dependencies of `tiktoken`, but we will not be leveraging them today."
      ],
      "metadata": {
        "id": "6l4CqoEDl7ks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken requests cohere openai -qU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_gepPv1Qdj_",
        "outputId": "09ecd75b-1f05-43bf-e4fd-76e972c73c61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.9/51.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.4/223.4 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First things first - let's download our dataset!\n",
        "\n",
        "We'll leverage the `requests` library to do this - and then we will split our resultant data into a `train` and `val` set."
      ],
      "metadata": {
        "id": "70hSjXmZmCt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "\n",
        "current_path = \"/data/shakespeare\"\n",
        "data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "\n",
        "if not os.path.exists(current_path):\n",
        "    os.makedirs(current_path)\n",
        "\n",
        "# download the tiny shakespeare dataset\n",
        "input_file_path = os.path.join(os.path.dirname(current_path), 'input.txt')\n",
        "if not os.path.exists(input_file_path):\n",
        "\n",
        "    with open(input_file_path, 'w') as f:\n",
        "        f.write(requests.get(data_url).text)\n",
        "\n",
        "with open(input_file_path, 'r') as f:\n",
        "    data = f.read()\n",
        "\n",
        "n = len(data)\n",
        "train_data = data[:int(n*0.9)]\n",
        "val_data = data[int(n*0.9):]"
      ],
      "metadata": {
        "id": "T7qRWArUNiZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's get our `tokenizers` dependency so we can train a tokenizer on our data."
      ],
      "metadata": {
        "id": "wU9BG2CymU-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers -qU"
      ],
      "metadata": {
        "id": "gFnrwKpQPsYh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "600d41c7-3f68-4c9c-e81d-58714d2217c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be training a \"byte-pair-encoding\" or \"BPE\" tokenizer. If you'd like to read more, you can find it [here](https://en.wikipedia.org/wiki/Byte_pair_encoding).\n",
        "\n",
        "Let's work through an example of what Byte-Pair Encoding (BPE) is doing, exactly, from this wonderful example provided by [Hugging Face](https://huggingface.co/docs/transformers/main/tokenizer_summary#byte-pair-encoding-bpe).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rmWXE5ctma9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is BPE?\n",
        "\n",
        "First, we need to do a step called \"pre-tokenization\", which is - as it sounds - a tokenization step that occurs before we tokenize.\n",
        "\n",
        "The essential idea of BPE is that we need to understand common words and \"byte-pairs\" in them. So, in order to find \"common words\" we first need to find...words!\n",
        "\n",
        "Let's take the following text and break it apart into its word components.\n",
        "\n",
        "\n",
        "```\n",
        "After pre-tokenization, a set of unique words has been created and the frequency with which each word occurred in the training data has been determined. Next, BPE creates a base vocabulary consisting of all symbols that occur in the set of unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. It does so until the vocabulary has attained the desired vocabulary size. Note that the desired vocabulary size is a hyperparameter to define before training the tokenizer.\n",
        "```\n",
        "\n",
        "A naive way to do this would just be by splitting on spaces...and that is indeed what technique was used in GPT-2."
      ],
      "metadata": {
        "id": "GLecDiHbogvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"\"\"\n",
        "After pre-tokenization, a set of unique words has been created and the frequency with which each word occurred in the training data has been determined. Next, BPE creates a base vocabulary consisting of all symbols that occur in the set of unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. It does so until the vocabulary has attained the desired vocabulary size. Note that the desired vocabulary size is a hyperparameter to define before training the tokenizer.\n",
        "\"\"\"\n",
        "\n",
        "naive_word_list = input_text.split()"
      ],
      "metadata": {
        "id": "m34NDAGCpiz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can count our words and get their frequency."
      ],
      "metadata": {
        "id": "hR8k-2bopqjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "vocab_and_frequencies = defaultdict(int)\n",
        "\n",
        "for word in naive_word_list:\n",
        "  vocab_and_frequencies[\" \".join(list(word))] += 1\n",
        "\n",
        "sorted(vocab_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_201bSQpvqD",
        "outputId": "91790512-18ec-47ae-ea18-63145502ac0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('t h e', 8), ('a', 4), ('o f', 4), ('v o c a b u l a r y', 4), ('h a s', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's find our \"base vocabulary\", which is going to be each symbol present in our original dataset."
      ],
      "metadata": {
        "id": "NckufSxxp-w5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Tuple, List, Set\n",
        "\n",
        "def find_vocabulary_size(current_vocab: Dict[str, int]) -> int:\n",
        "  vocab = set()\n",
        "\n",
        "  for word in current_vocab.keys():\n",
        "    for subword in word.split():\n",
        "      vocab.add(subword)\n",
        "\n",
        "  return len(vocab)"
      ],
      "metadata": {
        "id": "BNcjzjDvvKjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_vocabulary_size(vocab_and_frequencies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pf3kCf-WvdBL",
        "outputId": "dbc50b42-0345-4aa2-af9d-98ce769c6043"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, there are 36 symbols in our base vocabulary. Let's convert our data into a form where we can capture each symbol separately."
      ],
      "metadata": {
        "id": "VoMq7GhKqf7p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can start constructing our pairs. We will look at all the pairs of symbols as they appear and take into consideration their frequency in our corpus."
      ],
      "metadata": {
        "id": "OGxrHYmftDTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_pairs_and_frequencies(current_vocab: Dict[str, int]) -> Dict[str, int]:\n",
        "  pairs = {}\n",
        "\n",
        "  for word, frequency in current_vocab.items():\n",
        "    symbols = word.split()\n",
        "\n",
        "    for i in range(len(symbols) - 1):\n",
        "      pair = (symbols[i], symbols[i + 1])\n",
        "      current_frequency = pairs.get(pair, 0)\n",
        "      pairs[pair] = current_frequency + frequency\n",
        "\n",
        "  return pairs"
      ],
      "metadata": {
        "id": "sTwvfTAErQN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs_and_frequencies = find_pairs_and_frequencies(vocab_and_frequencies)"
      ],
      "metadata": {
        "id": "FudOaKmYv9-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(pairs_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGIJfkk7wFYw",
        "outputId": "903ecff7-9de5-45e3-e1c4-63c7a083af5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('t', 'h'), 11),\n",
              " (('i', 'n'), 10),\n",
              " (('r', 'e'), 8),\n",
              " (('h', 'e'), 8),\n",
              " (('a', 't'), 7)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the frequent pairs - we can merge those pairs into a single token.\n",
        "\n",
        "Let's see how this process looks in code."
      ],
      "metadata": {
        "id": "OqORqdzwsZ6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def merge_vocab(most_common_pair: Tuple[str], current_vocab: Dict[str, int]) -> Dict[str, int]:\n",
        "  vocab_out = {}\n",
        "\n",
        "  pattern = re.escape(' '.join(most_common_pair))\n",
        "  replacement = ''.join(most_common_pair)\n",
        "\n",
        "  for word_in in current_vocab:\n",
        "      word_out = re.sub(pattern, replacement, word_in)\n",
        "      vocab_out[word_out] = current_vocab[word_in]\n",
        "\n",
        "  return vocab_out"
      ],
      "metadata": {
        "id": "L7ohHm2kshoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_vocab_and_frequencies = merge_vocab(\n",
        "    sorted(pairs_and_frequencies.items(), key = lambda x: x[1], reverse=True)[0][0],\n",
        "    vocab_and_frequencies\n",
        ")"
      ],
      "metadata": {
        "id": "Ab760KKuwzZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(new_vocab_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0XtvLbpxbSx",
        "outputId": "10db54fe-4013-4afd-c084-f8160d84b5e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('th e', 8), ('a', 4), ('o f', 4), ('v o c a b u l a r y', 4), ('h a s', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After one merge, we can see that `t h` has been converted to `th`!\n",
        "\n",
        "Let's see how that impacted our vocabulary."
      ],
      "metadata": {
        "id": "9DPkBzj2u-me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "find_vocabulary_size(new_vocab_and_frequencies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO_xegCtxjQf",
        "outputId": "7babce8c-4198-497f-f9f8-2951f5406527"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that our vocabulary has increased by 1 as we've added the `th` symbol to it!\n",
        "\n",
        "In essence, BPE will continue to do this process until your desired vocabulary size (a hyper-parameter) is met!"
      ],
      "metadata": {
        "id": "o3M13D60xzZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Our Tokenizer\n",
        "\n",
        "Now that we have some background on how BBPE works, lets move on to training our tokenizer for our model!\n",
        "\n",
        "Let's walk through the steps we'll take:\n",
        "\n",
        "1. Initialize our `Tokenizer` with a `BPE` model. Be sure to include the `unk_token`.\n",
        "\n",
        "  - [`Tokenizer`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizer)\n",
        "  - [`Models`](https://huggingface.co/docs/tokenizers/api/models#models)\n",
        "\n",
        "2. We'll include a normalizer, applied at the sequence level, and we'll use `NFD()` to do so. More reading on Unicode Normalization Forms [here](https://unicode.org/reports/tr15/#Normalization_Forms_Table).\n",
        "\n",
        "  - [`NFD()`](https://huggingface.co/docs/tokenizers/api/normalizers#tokenizers.normalizers.NFD)\n",
        "\n",
        "3. We'll also add our `ByteLevel()` pre-tokenizer, and our `ByteLevelDecoder()` decoder.\n",
        "\n",
        "  - [`ByteLevel()`](https://huggingface.co/docs/tokenizers/api/pre-tokenizers#tokenizers.pre_tokenizers.ByteLevel)\n",
        "  - [`ByteLevelDecoder()`](https://huggingface.co/docs/tokenizers/api/decoders#tokenizers.decoders.ByteLevel)"
      ],
      "metadata": {
        "id": "BePYCbHly02H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
        "from tokenizers.normalizers import NFD, Sequence\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer.normalizer = Sequence([NFD()])\n",
        "tokenizer.pre_tokenizer = ByteLevel()\n",
        "tokenizer.decoder = ByteLevelDecoder()"
      ],
      "metadata": {
        "id": "OrztE09OPosB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll want to add some special tokens to our tokenizer to ensure in has access to common token patterns.\n",
        "\n",
        "Let's use the following:\n",
        "\n",
        "- `\"<s>\"`    : bos_token - beginning of sequence token\n",
        "- `\"</s>\"`   : eos_token - end of sequence token\n",
        "- `\"<pad>\"`  : padding_token - token used to pad sequences\n",
        "- `\"<unk>\"`  : unk_token - token used to represent unknown tokens.\n",
        "- `\"<mask>\"` : mask_token - token used to mask parts of our sequence\n",
        "\n",
        "We're also going to set a target vocabulary of 50,000 tokens."
      ],
      "metadata": {
        "id": "dDqkNNdM1KsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = BpeTrainer(\n",
        "    vocab_size=50000,\n",
        "    show_progress=True,\n",
        "    special_tokens=[\n",
        "      \"<s>\",\n",
        "      \"<pad>\",\n",
        "      \"</s>\",\n",
        "      \"<unk>\",\n",
        "      \"<mask>\"\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "x9iQVhN3P3RN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nothing left to do but point it at our data-source and let it train!\n",
        "\n",
        "We'll use the `.train()` method to accomplish this task.\n",
        "\n",
        "> NOTE: Pay attention to the desired inputs of the `.train()` method.\n",
        "\n",
        "- [`Tokenizer.train()`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizers.Tokenizer.train)"
      ],
      "metadata": {
        "id": "yQ8X9vZe2Fyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.train(files=[input_file_path], trainer=trainer)"
      ],
      "metadata": {
        "id": "LinLHotSP7gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can save our tokenizer - and then load it as a `GPT2Tokenizer` through the Hugging Face Library!"
      ],
      "metadata": {
        "id": "V2JNYiqB2qKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = '/content/tokenizer'\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "tokenizer.model.save(save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jk6QjDGHQy2K",
        "outputId": "56c30454-38dc-4c9d-dcfd-8044c0239cd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/tokenizer/vocab.json', '/content/tokenizer/merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -qU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOOlbggdRFrN",
        "outputId": "cda96241-5159-4b1b-c39d-ea4031141a0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(save_path, unk_token=\"[UNK]\")"
      ],
      "metadata": {
        "id": "us1vofdhQ45C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how it tokenizes our inputs!"
      ],
      "metadata": {
        "id": "0-Bnq7lV2xWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = \"Hark, my name be Romeo! I am but a beautiful summer's day!\""
      ],
      "metadata": {
        "id": "dnYnFa3fTRLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentence = tokenizer.tokenize(input_sentence)\n",
        "tokenized_sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSHY5VufRbBj",
        "outputId": "67d0c201-e096-4061-b992-7e257f0e0783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hark',\n",
              " ',',\n",
              " 'Ġmy',\n",
              " 'Ġname',\n",
              " 'Ġbe',\n",
              " 'ĠRomeo',\n",
              " '!',\n",
              " 'ĠI',\n",
              " 'Ġam',\n",
              " 'Ġbut',\n",
              " 'Ġa',\n",
              " 'Ġbeautiful',\n",
              " 'Ġsummer',\n",
              " \"'s\",\n",
              " 'Ġday',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_tokens = tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
        "encoded_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZrWzQQlTU41",
        "outputId": "d0572b81-1633-464a-fff6-b58ae0520d38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[12077, 9, 124, 637, 121, 826, 5, 87, 295, 219, 72, 9113, 2999, 141, 511, 5]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_tokens = tokenizer.decode(encoded_tokens, clean_up_tokenization_spaces=False)\n",
        "decoded_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oS6lE-NLRnzk",
        "outputId": "e9b2a346-c8b4-452b-a487-b6f57517bb38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hark, my name be Romeo! I am but a beautiful summer's day!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing Dataset\n",
        "\n",
        "Now that we have trained our tokenizer - let's create a dataset we can leverage with the `nanoGPT` library.\n",
        "\n",
        "We'll simply encode our training and validation data - and then save them in binary files for later!\n",
        "\n",
        "> NOTE: Pay attention to the format you want your dataset in. We want ids, which means we want to use the [`.encode()`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizers.Tokenizer.encode) method of our tokenizer."
      ],
      "metadata": {
        "id": "ji3sF-rA21YH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ids = tokenizer.encode(train_data)\n",
        "val_ids = tokenizer.encode(val_data)\n",
        "print(f\"train has {len(train_ids):,} tokens\")\n",
        "print(f\"val has {len(val_ids):,} tokens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "calHML6JPnCU",
        "outputId": "c6153252-b5ea-4f51-f83f-a87414561ed9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 291,284 tokens\n",
            "val has 34,223 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# export to bin files\n",
        "data_path = \"/data/shakespeare/\"\n",
        "\n",
        "train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "train_ids.tofile(os.path.join(os.path.dirname(data_path), 'train.bin'))\n",
        "val_ids.tofile(os.path.join(os.path.dirname(data_path), 'val.bin'))"
      ],
      "metadata": {
        "id": "nKJ1KqiiPkRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at our first 100 training tokens to see what format they are in!"
      ],
      "metadata": {
        "id": "DFbbvIi7xsgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ids[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9z7ia8AxqEn",
        "outputId": "e9c282cc-5277-4624-b72c-a495d9c8b90e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  21,  388,  876,   13,   68, 6804,  373,  153, 2501,  622, 2092,\n",
              "          9,  496,  136,  433,   11,   68,   68,   16,   89,   13,   68,\n",
              "         34, 7882,    9,  433,   11,   68,   68,   21,  388,  876,   13,\n",
              "         68,   40,   73,  252,  227, 3778, 1304,  103,  781,  351,  103,\n",
              "       7504,   15,   68,   68,   16,   89,   13,   68,   33,   97, 5790,\n",
              "         11, 3778,   11,   68,   68,   21,  388,  876,   13,   68,   21,\n",
              "        388,    9,  104,  330, 3317, 1177,  145, 3563, 1766,  103,   80,\n",
              "       1006,   11,   68,   68,   16,   89,   13,   68, 7797,  330,  486,\n",
              "          9,  153,  330,  486,   11,   68,   68,   21,  388,  876,   13,\n",
              "         68], dtype=uint16)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training The Model\n",
        "\n",
        "Now that we have our tokenized dataset, let's get to training our model!\n",
        "\n",
        "We have a lot of set-up to do before we click \"`.train()`\", so let's jump right into it!\n",
        "\n",
        "First, let's literally jump into the `nanoGPT` repository we cloned earlier."
      ],
      "metadata": {
        "id": "c0I3VrRC3XIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd nanoGPT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUU2jaalUdqm",
        "outputId": "b49a6386-68da-4b09-a6f8-0b4ee7490584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nanoGPT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll do some critical imports."
      ],
      "metadata": {
        "id": "13p1e8sa3k0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# from the local repo\n",
        "from model import GPTConfig, GPT"
      ],
      "metadata": {
        "id": "weNR37BwUYNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyper-Parameters\n",
        "\n",
        "We have a laundry list of hyper-parameters to set up - let's walk through them and what they mean."
      ],
      "metadata": {
        "id": "kY_vWZG-3uM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### I/O\n",
        "\n",
        "- `out_dir` - simple enough, this is the output directory where our checkpoints are saved"
      ],
      "metadata": {
        "id": "OykCjVQK5EX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out_dir = 'out'"
      ],
      "metadata": {
        "id": "viM3qlWt5PVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initialization\n",
        "\n",
        "Since we're training from scratch, we'll use `init_from = 'scratch'`."
      ],
      "metadata": {
        "id": "A5iwwrNL5H4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_from = 'scratch'"
      ],
      "metadata": {
        "id": "OK1z2m3C312T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Eval and Logging\n",
        "\n",
        "- `eval_interval` - this is the number of steps between evaluation stages, we'll want to see this ~`250`. Our model will be incredibly prone to over-fitting, and this will let us monitor with relative frequency.\n",
        "- `log_interval` - this is how often our training progress will log. You can set this ~`10`. It's dealer's choice, really.\n",
        "- `eval_iters` - this is how *many* iterations we want to evaluate for.\n",
        "- `eval_only` - this would evaluate our model - but not train it. We'll leave this as `False` for now.\n",
        "- `always_save_checkpoint` - this will always save our most recent checkpoint, regardless of metrics. For this example, we'll set this to `True`."
      ],
      "metadata": {
        "id": "2YlolKOj4_dE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_interval = 250\n",
        "eval_iters = 200\n",
        "log_interval = 10\n",
        "eval_only = False\n",
        "always_save_checkpoint = True"
      ],
      "metadata": {
        "id": "MbFN5Ltq4_mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset\n",
        "\n",
        "We can set our dataset here - we'll use the one we created earlier!"
      ],
      "metadata": {
        "id": "a488zaF_4zQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = 'shakespeare'"
      ],
      "metadata": {
        "id": "_QC7vWXC40Hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Typical Hyper-Parameters\n",
        "\n",
        "- `gradient_accumulation_steps` - we can use gradient accumulation to \"simulate\" larger batch sizes by combining multiple different optimization steps together, without needing the additional memory for large batch sizes. We don't need to worry so much about this for the toy problem - but this hyper-parameter can be configured for larger training runs. [Here](https://lightning.ai/blog/gradient-accumulation/) is some great reading on the topic.\n",
        "- `batch_size` - Typical batch_size - the larger the merrier (up to a point) we'll be using `16` to ensure we do not exceed the memory quota of our GPU.\n",
        "- `block_size` - this can be thought of as another term for the `context window` of our model. Since our model cannot take variable length inputs - we use this to set all inputs to our desired size. We'll use a value of `512` to ensure speedy training."
      ],
      "metadata": {
        "id": "XP9rBgGc426Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_accumulation_steps = 1\n",
        "batch_size = 16\n",
        "block_size = 512"
      ],
      "metadata": {
        "id": "EM_ybLPP43Pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Architecture\n",
        "\n",
        "- `n_layer` - this is the number of decoder layers we will use in our model. More would be considered better (up to a point) and the original GPT-2 paper uses `12`, but we will be using a truncated `6` for ease and speed of training.\n",
        "- `n_head` - this is the number of attention heads in each decoder layer!\n",
        "- `n_embd` - this is the embedding dimension of our model, this is analagous to our `model_d` from the previous notebook.\n",
        "- `dropout` - this sets our dropout value, since our model is small and going to be extremely prone to overfitting, consider setting this at a fairly aggresive level (`0.2` was used in the example training found in the notebook`).\n",
        "- `bias` - wether or not to use bias inside the LayerNorm/Linear layers.\n",
        "\n",
        "> NOTE: You need to ensure your `n_embd` is cleanly divided by your `n_head`. That is to say:\n",
        ">\n",
        "> `n_embd % n_head == 0`."
      ],
      "metadata": {
        "id": "UZ-8bDIY45GS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_layer = 6\n",
        "n_head = 6\n",
        "n_embd = 516\n",
        "dropout = 0.2\n",
        "bias = False"
      ],
      "metadata": {
        "id": "gMyyDBxB6k4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optimizer Hyper-Parameters\n",
        "\n",
        "Basic Optimizer Hyper-Parameters:\n",
        "\n",
        "- `learning_rate` - it's our learning rate! We'll want to set this fairly high ~`1e-3` since we're training on such a small dataset.\n",
        "- `max_iters` - how many iterations do we train for. More iters means longer training times. Feel free to tinker with this value! `5000` is a great place to start.\n",
        "\n",
        "Learning Rate Decay Settings:\n",
        "\n",
        "- `decay_lr` - set decay flag\n",
        "- `weight_Decay` - how much to decay lr by\n",
        "- `lr_decay_iters` - should be set to ~max_iters.\n",
        "- `min_lr` - the minimum lr, should be ~ lr / 10\n",
        "\n",
        "Clipping and Warmup:\n",
        "\n",
        "- `grad_clip` - value to clip gradients to. useful for preventing vanishing gradients.\n",
        "- `warmup_iters` - how many iterations to warmup for. Warmup is useful to allow your training to slowly warmup. It will use a low lr for a number of steps to avoid any massive initial spikes. Since we're training a very small model - we can avoid using many wamrup steps.\n",
        "\n",
        "> NOTE: Many learnings taken from the [Chincilla paper](https://arxiv.org/pdf/2203.15556.pdf) for selecting default or appropriate values."
      ],
      "metadata": {
        "id": "3NWDTaAz7gwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adamw optimizer\n",
        "learning_rate = 1e-3\n",
        "max_iters = 5_000\n",
        "beta1 = 0.9\n",
        "beta2 = 0.99\n",
        "\n",
        "# lr decay settings\n",
        "decay_lr = True\n",
        "weight_decay = 1e-1\n",
        "lr_decay_iters = 5_000\n",
        "min_lr = 1e-4\n",
        "\n",
        "# clipping and warmup\n",
        "grad_clip = 1.0\n",
        "warmup_iters = 100"
      ],
      "metadata": {
        "id": "qe-669jwUptI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These hyper-parameters are necessary to set given the task we're training and given the environment we're training in."
      ],
      "metadata": {
        "id": "ucldc4mz9yeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "backend = 'nccl'\n",
        "device = 'cuda'\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
        "compile = True\n",
        "# -----------------------------------------------------------------------------\n",
        "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
        "config = {k: globals()[k] for k in config_keys}\n",
        "# -----------------------------------------------------------------------------\n",
        "master_process = True\n",
        "seed_offset = 0\n",
        "ddp_world_size = 1\n",
        "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "os.makedirs(out_dir, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHiGlMOp8Nux",
        "outputId": "49097c98-cbc6-4bd8-a30d-bbefe3e10a93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens per iteration will be: 8,192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Torch Settings\n",
        "\n",
        "We need to set a few `torch` settings, including the seed, to allow us to train correctly on our GPU.\n",
        "\n",
        "Not much is required for us to understand here - these are just necessary lines of code. Boilerplate."
      ],
      "metadata": {
        "id": "eKmdfbye-BNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
      ],
      "metadata": {
        "id": "yh34QGD6VARU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataloader\n",
        "\n",
        "This block will:\n",
        "\n",
        "1. Set the data path\n",
        "2. Load the dataset we tokenized earlier from the `.bin` we saved\n",
        "3. Define a `get_batch` function that will return us a random section of our data as well as a the corresponding \"label\" for that data and move it to the GPU for easy use inside our training loop."
      ],
      "metadata": {
        "id": "gKeNwYaZ-Zoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = os.path.join('/data', dataset)\n",
        "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "tOjaPyJpVEgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at what an example of our batches would look like.\n",
        "\n",
        "To remind ourselves:\n",
        "\n",
        "- `train_data` - has ~2.9 million entries\n",
        "- `block_size` - is 512\n",
        "- `batch_size` - is 16"
      ],
      "metadata": {
        "id": "1Z7vMU34yRbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ix = torch.randint(len(train_data) - block_size, (batch_size,))\n",
        "x = torch.stack([torch.from_numpy((train_data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "y = torch.stack([torch.from_numpy((train_data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])"
      ],
      "metadata": {
        "id": "9_-Y5RZ-yX2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Our randomly selected indices were: {ix}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JDxXph4yh2g",
        "outputId": "27fd09cf-8f21-4013-80f3-d55c00f9d88b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our randomly selected indices were: tensor([ 99775, 155569, 263696,  32920,  52919, 231541, 153767, 229238, 136782,\n",
            "        263618,  39008,  14208,  39429, 189430, 194466,  76798])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The first 10 elements of `x` at the first randomly selected index is:\\n{x[0][:10]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAzHJH-kzK5E",
        "outputId": "40289227-7659-42d0-edb0-008961057d08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first 10 elements of `x` at the first randomly selected index is:\n",
            "tensor([   68,    16,    81,  2358, 19949,   116,   172,  1280,     9,    68])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The first 10 elements of `y` at the first randomly selected index is:\\n{y[0][:10]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbaF6v8ezkWn",
        "outputId": "bed6fcf8-0906-4720-809b-3b199ce60d05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first 10 elements of `y` at the first randomly selected index is:\n",
            "tensor([   16,    81,  2358, 19949,   116,   172,  1280,     9,    68,    16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the first component selects a random index from our training data (accounting for our block size)"
      ],
      "metadata": {
        "id": "N62oDfdWy0XJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Initialization of Model\n",
        "\n",
        "Here we init our number of iterations as 0, and our best val loss as a very high number."
      ],
      "metadata": {
        "id": "EbDlW-68_atH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num = 0\n",
        "best_val_loss = 1e9"
      ],
      "metadata": {
        "id": "6hsepdVBVzQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtain our vocab size from our trained tokenizer."
      ],
      "metadata": {
        "id": "A4Uj9qBI_vXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
        "meta_vocab_size = tokenizer.vocab_size\n",
        "meta_vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m53DcCdFV0_a",
        "outputId": "4e585d96-f35c-4995-c809-32a79ccec7a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20099"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create our model args dict."
      ],
      "metadata": {
        "id": "V7bcNelYARmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
        "                  bias=bias, vocab_size=None, dropout=dropout)"
      ],
      "metadata": {
        "id": "JfIWEbanV7ZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate our model with the provided `model_args`.\n",
        "\n",
        "These are derived from the hyper-parameters we set above."
      ],
      "metadata": {
        "id": "2WWcbkiCAUI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if init_from == 'scratch':\n",
        "    print(\"Initializing a new model from scratch\")\n",
        "    if meta_vocab_size is None:\n",
        "        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
        "    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Xly4iA0V-vF",
        "outputId": "dbfcf75a-249c-4162-b07d-141d555c7483"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing a new model from scratch\n",
            "number of parameters: 29.55M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There we go! If you used the default values - you should have a model with 29.55M parameters!\n",
        "\n",
        "Let's set our block_size to the correct size as determined in our configuration steps."
      ],
      "metadata": {
        "id": "BpViOsxLAl6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if block_size < model.config.block_size:\n",
        "    model.crop_block_size(block_size)\n",
        "    model_args['block_size'] = block_size"
      ],
      "metadata": {
        "id": "TrEawNxdWRhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can look at our model in all its glory!"
      ],
      "metadata": {
        "id": "eRgguPLKAuZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaE3KSTnAtJs",
        "outputId": "76862c66-60ec-40c4-c496-f82160ee12a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(20099, 516)\n",
              "    (wpe): Embedding(512, 516)\n",
              "    (drop): Dropout(p=0.2, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-5): 6 x Block(\n",
              "        (ln_1): LayerNorm()\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=516, out_features=1548, bias=False)\n",
              "          (c_proj): Linear(in_features=516, out_features=516, bias=False)\n",
              "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=516, out_features=2064, bias=False)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): Linear(in_features=2064, out_features=516, bias=False)\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=516, out_features=20099, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture Breakdown\n",
        "\n",
        "Now that we've built our model - let's look at each component and see how it works!"
      ],
      "metadata": {
        "id": "XiTlDpQQaq0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization\n",
        "\n",
        "We've already talked about tokenization - so let's expand on it a bit here:\n",
        "\n",
        "![image](https://i.imgur.com/oYuAayM.png)\n",
        "\n",
        "As you can see - our sentences are simply converted from a string to a sequence of numeric values - each which represents a word or subword!"
      ],
      "metadata": {
        "id": "SHwAK0lfaxtY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding Layers\n",
        "\n",
        "The first step will be do convert our tokenized sequence of inputs into an embedding vector. This allows use to understand a rich amount of information about input sequences and their semantic meanings.\n",
        "\n",
        "As the embedding layer will be training along side the rest of the model - it will allow us to have an excellent vector-representation of the tokens in our dataset.\n",
        "\n",
        "Let's see how it looks in code!"
      ],
      "metadata": {
        "id": "N_4mk2yKbFF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "```"
      ],
      "metadata": {
        "id": "ntf8i9EabKwD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see a visual example!\n",
        "\n",
        "![image](https://i.imgur.com/Q8fiuw2.png)"
      ],
      "metadata": {
        "id": "qh9pH_4dbWan"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional Encoding\n",
        "\n",
        "We need to impart information about where each token is in the sequence, but we aren't using any recurrence or convolutions - the easiest way to encode positional information is to inject positional information into our input embeddings.\n",
        "\n",
        "We can use two different methods to obtain our positional encodings - either a learned positional encoding, or a calculated positional encoding.\n",
        "\n",
        "Let's look at the code from the example used in `nanoGPT`, and also an example of code that uses a calculated positional encoding.\n",
        "\n"
      ],
      "metadata": {
        "id": "NNPEiOejbcBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "wpe = nn.Embedding(config.block_size, config.n_embd)\n",
        "```"
      ],
      "metadata": {
        "id": "AaJQVo0ibujN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A familiar face! In this case, our positional encodings are quite similar to our embeddings layer - but we're setting the dimensionality to care about the `block_size` (context window) as opposed to the `vocab_size`!"
      ],
      "metadata": {
        "id": "2QEmKK8Dbybh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another method we could use is the calculated positional embeddings - which could be implemented like so:"
      ],
      "metadata": {
        "id": "KRFQN34qfnZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model: int, seq_len: int, dropout: float, verbose=False) -> None:\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.seq_len = seq_len\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.verbose=verbose\n",
        "\n",
        "    positional_embeddings = torch.zeros(seq_len, d_model)\n",
        "    positional_sequence_vector = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "    positional_model_vector = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "    positional_embeddings[:, 0::2] = torch.sin(positional_sequence_vector * positional_model_vector)\n",
        "    positional_embeddings[:, 1::2] = torch.cos(positional_sequence_vector * positional_model_vector)\n",
        "    positional_embeddings = positional_embeddings.unsqueeze(0)\n",
        "\n",
        "    self.register_buffer('positional_embeddings', positional_embeddings)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + (self.positional_embeddings[:, :x.shape[1], :]).requires_grad_(False)\n",
        "    if self.verbose:\n",
        "      print(f\"Positional Encodings (1st 5 elements): {x[0, :5]}\")\n",
        "    return self.dropout(x)\n",
        "```"
      ],
      "metadata": {
        "id": "wCrtpp6pfsgC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we're implementing this set of equations from \"Attention is All You Need\"!\n",
        "\n",
        "![image](https://i.imgur.com/UhAJ0H0.png)"
      ],
      "metadata": {
        "id": "eODsnnADfxGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at this process visually to better understand:\n",
        "\n",
        "![image](https://i.imgur.com/JKwbESf.png)"
      ],
      "metadata": {
        "id": "k4Wh1cAxlY1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention Mechanism\n",
        "\n",
        "Now we get to the \"meat\" of the model - the scaled dot-product attention mechanism!\n",
        "\n",
        "Let's first take a look at how it's implemented in code!"
      ],
      "metadata": {
        "id": "E3Zfr5tAgF33"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                        .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        if self.flash:\n",
        "            # efficient attention using Flash Attention CUDA kernels\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
        "        else:\n",
        "            # manual implementation of attention\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "```"
      ],
      "metadata": {
        "id": "f5zI82M8gTfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's discuss a few key components and describe how they align with the attention mechanism outlined in the paper!\n",
        "\n",
        "First:\n",
        "\n",
        "```python\n",
        "att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "```\n",
        "\n",
        "This is the where we find our \"scaled dot-product\" attention scores!\n",
        "\n",
        "Second:\n",
        "\n",
        "```python\n",
        "att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "```\n",
        "\n",
        "This is where we're applying our \"causal mask\" that ensures we're only ever looking back, and can't look forward past our current token.\n",
        "\n",
        "Third:\n",
        "\n",
        "```python\n",
        "att = F.softmax(att, dim=-1)\n",
        "```\n",
        "\n",
        "This is where we're computing the soft-max of our attention scores.\n",
        "\n",
        "Fourth:\n",
        "\n",
        "```python\n",
        "y = att @ v\n",
        "```\n",
        "\n",
        "This is where we're doing the last MatMul between our computed soft-max attention scores and our value vector!"
      ],
      "metadata": {
        "id": "_a7c1OnbgdW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://i.imgur.com/lJOWXcE.png)"
      ],
      "metadata": {
        "id": "oM2VtkhLkfaZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Back to Initializing Our Model"
      ],
      "metadata": {
        "id": "TOkvFxqClDDo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll set up our GradScaler - more information on this process [here](https://pytorch.org/docs/stable/amp.html#gradient-scaling)."
      ],
      "metadata": {
        "id": "LzoEY6gcBOSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"
      ],
      "metadata": {
        "id": "BNUThRt4WT5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's set up our optimizer below. Be sure to include the correct values. You can check the `model.py` file for more information on what is expected in the `configure_optimizers` method [here](https://github.com/karpathy/nanoGPT/blob/eba36e84649f3c6d840a93092cb779a260544d08/model.py#L263C85-L263C85)."
      ],
      "metadata": {
        "id": "6Zs5Hcf9BBUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = model.configure_optimizers(\n",
        "    weight_decay,\n",
        "    learning_rate,\n",
        "    (beta1, beta2),\n",
        "    device_type\n",
        ")\n",
        "\n",
        "checkpoint = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YesGeUnoWViL",
        "outputId": "d05459ae-4a9d-44dc-b0c8-a1d597089883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num decayed parameter tensors: 26, with 29,805,708 parameters\n",
            "num non-decayed parameter tensors: 13, with 6,708 parameters\n",
            "using fused AdamW: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can compile our model!\n",
        "\n",
        "If you're using the T4 or V100 instance of Colab - this will not provide a signficant speed-up, but if you're using Ampere architecture (A100) you should notice a significant difference between the compiled and uncompiled model.\n",
        "\n",
        "Read more about `torch.compile()` [here](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)."
      ],
      "metadata": {
        "id": "ZF5YWJoKB4og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model) # requires PyTorch 2.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0FNU0T0WXdI",
        "outputId": "bacfa39c-d7b9-4055-b430-e059ad3f3e1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compiling the model... (takes a ~minute)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll set up our loss estimation function here, which will help us estimate an arbitrarily accurate loss over either training or validation data by using many batches.\n",
        "\n",
        "You'll notice that we quickly convert the model into `.eval()` model and then back to `.train()` mode."
      ],
      "metadata": {
        "id": "p6lRcVsZCXRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "lUB5zVLVWbhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating our LR Scheduler\n",
        "\n",
        "Beyond just slowly reducing our learning rate over time - we can use an LR Scheduler to allow us to move our learning according to a desired pattern.\n",
        "\n",
        "We will use a \"cosine with warmup\" schedule and our learning rate, thusly, will follow this pattern:\n",
        "\n",
        "![img](https://i.imgur.com/KoFEl0b.png)\n",
        "\n",
        "There are many different schedulers, and many different ways to handle learning rate, and you can read about just a few of them [here](https://d2l.ai/chapter_optimization/lr-scheduler.html)!"
      ],
      "metadata": {
        "id": "fLsOpaACDDkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)"
      ],
      "metadata": {
        "id": "7-mNpWBSWdHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to set some specific values in our env to allow training in Colab."
      ],
      "metadata": {
        "id": "cqFePCZmE1Lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!export LC_ALL=\"en_US.UTF-8\"\n",
        "!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
        "!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
        "!ldconfig /usr/lib64-nvidia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7nDL6s4YT6E",
        "outputId": "9e793721-22c8-4a94-aea2-dde66456fa6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Training Loop\n",
        "\n",
        "Now we can finally grab our first batch and set our initial time to calculate how long our iterations are taking!"
      ],
      "metadata": {
        "id": "Nhqmxeo0Eg0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = get_batch('train')\n",
        "t0 = time.time()\n",
        "local_iter_num = 0\n",
        "raw_model = model\n",
        "running_mfu = -1.0 # model flops utilization\n",
        "\n",
        "while True:\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0 and master_process:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = losses['val']\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': iter_num,\n",
        "                    'best_val_loss': best_val_loss,\n",
        "                    'config': config,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch('train')\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0 and master_process:\n",
        "        # get loss as float. note: this is a CPU-GPU sync point\n",
        "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5: # let the training loop settle a bit\n",
        "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHbyEapRWmpc",
        "outputId": "2c041fdb-0571-4ae7-9808-7ad31b51c6cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 9.9352, val loss 9.9273\n",
            "iter 0: loss 9.9333, time 72385.72ms, mfu -100.00%\n",
            "iter 10: loss 8.3523, time 218.01ms, mfu 2.36%\n",
            "iter 20: loss 7.3770, time 217.55ms, mfu 2.36%\n",
            "iter 30: loss 6.4399, time 218.19ms, mfu 2.36%\n",
            "iter 40: loss 5.8015, time 219.20ms, mfu 2.36%\n",
            "iter 50: loss 5.7303, time 219.79ms, mfu 2.36%\n",
            "iter 60: loss 5.5227, time 218.98ms, mfu 2.36%\n",
            "iter 70: loss 5.2348, time 221.42ms, mfu 2.36%\n",
            "iter 80: loss 5.1075, time 222.83ms, mfu 2.35%\n",
            "iter 90: loss 4.9910, time 220.26ms, mfu 2.35%\n",
            "iter 100: loss 4.5895, time 220.48ms, mfu 2.35%\n",
            "iter 110: loss 4.6115, time 226.61ms, mfu 2.34%\n",
            "iter 120: loss 4.5511, time 226.96ms, mfu 2.34%\n",
            "iter 130: loss 4.5340, time 228.43ms, mfu 2.33%\n",
            "iter 140: loss 4.4540, time 230.10ms, mfu 2.32%\n",
            "iter 150: loss 4.4588, time 230.59ms, mfu 2.31%\n",
            "iter 160: loss 4.4423, time 231.41ms, mfu 2.30%\n",
            "iter 170: loss 4.3046, time 229.77ms, mfu 2.30%\n",
            "iter 180: loss 4.4322, time 234.02ms, mfu 2.29%\n",
            "iter 190: loss 4.2641, time 232.46ms, mfu 2.28%\n",
            "iter 200: loss 4.2333, time 238.14ms, mfu 2.27%\n",
            "iter 210: loss 4.2541, time 239.51ms, mfu 2.26%\n",
            "iter 220: loss 4.2477, time 240.94ms, mfu 2.25%\n",
            "iter 230: loss 4.1968, time 238.87ms, mfu 2.24%\n",
            "iter 240: loss 4.0941, time 241.08ms, mfu 2.23%\n",
            "step 250: train loss 3.9769, val loss 4.9525\n",
            "saving checkpoint to out\n",
            "iter 250: loss 4.0349, time 25010.32ms, mfu 2.01%\n",
            "iter 260: loss 4.0595, time 228.54ms, mfu 2.03%\n",
            "iter 270: loss 3.8779, time 229.63ms, mfu 2.05%\n",
            "iter 280: loss 4.0134, time 226.87ms, mfu 2.07%\n",
            "iter 290: loss 3.8799, time 227.41ms, mfu 2.09%\n",
            "iter 300: loss 3.9018, time 225.66ms, mfu 2.11%\n",
            "iter 310: loss 3.9823, time 229.61ms, mfu 2.13%\n",
            "iter 320: loss 3.8021, time 227.63ms, mfu 2.14%\n",
            "iter 330: loss 3.8000, time 228.57ms, mfu 2.15%\n",
            "iter 340: loss 3.8816, time 227.44ms, mfu 2.16%\n",
            "iter 350: loss 3.7371, time 227.93ms, mfu 2.17%\n",
            "iter 360: loss 3.6282, time 230.00ms, mfu 2.18%\n",
            "iter 370: loss 3.7428, time 229.87ms, mfu 2.19%\n",
            "iter 380: loss 3.7041, time 228.39ms, mfu 2.19%\n",
            "iter 390: loss 3.6977, time 230.68ms, mfu 2.20%\n",
            "iter 400: loss 3.4849, time 230.76ms, mfu 2.20%\n",
            "iter 410: loss 3.6687, time 232.47ms, mfu 2.20%\n",
            "iter 420: loss 3.5347, time 230.12ms, mfu 2.21%\n",
            "iter 430: loss 3.6264, time 233.07ms, mfu 2.21%\n",
            "iter 440: loss 3.5310, time 232.56ms, mfu 2.21%\n",
            "iter 450: loss 3.5382, time 232.51ms, mfu 2.21%\n",
            "iter 460: loss 3.5053, time 232.92ms, mfu 2.21%\n",
            "iter 470: loss 3.5572, time 235.63ms, mfu 2.21%\n",
            "iter 480: loss 3.3564, time 232.94ms, mfu 2.21%\n",
            "iter 490: loss 3.4856, time 235.68ms, mfu 2.21%\n",
            "step 500: train loss 3.3500, val loss 5.1336\n",
            "saving checkpoint to out\n",
            "iter 500: loss 3.3812, time 24875.43ms, mfu 1.99%\n",
            "iter 510: loss 3.5416, time 231.59ms, mfu 2.01%\n",
            "iter 520: loss 3.4406, time 234.83ms, mfu 2.03%\n",
            "iter 530: loss 3.3095, time 231.82ms, mfu 2.05%\n",
            "iter 540: loss 3.4713, time 228.73ms, mfu 2.07%\n",
            "iter 550: loss 3.3033, time 231.33ms, mfu 2.09%\n",
            "iter 560: loss 3.1901, time 231.40ms, mfu 2.10%\n",
            "iter 570: loss 3.1203, time 227.95ms, mfu 2.12%\n",
            "iter 580: loss 3.2158, time 231.99ms, mfu 2.13%\n",
            "iter 590: loss 3.1621, time 232.04ms, mfu 2.14%\n",
            "iter 600: loss 3.2119, time 232.65ms, mfu 2.14%\n",
            "iter 610: loss 3.1565, time 232.76ms, mfu 2.15%\n",
            "iter 620: loss 3.0997, time 230.74ms, mfu 2.16%\n",
            "iter 630: loss 3.2095, time 233.95ms, mfu 2.16%\n",
            "iter 640: loss 3.1999, time 230.89ms, mfu 2.17%\n",
            "iter 650: loss 3.1038, time 232.14ms, mfu 2.18%\n",
            "iter 660: loss 3.1274, time 231.40ms, mfu 2.18%\n",
            "iter 670: loss 3.0974, time 230.62ms, mfu 2.19%\n",
            "iter 680: loss 2.9744, time 231.03ms, mfu 2.19%\n",
            "iter 690: loss 3.0516, time 232.42ms, mfu 2.19%\n",
            "iter 700: loss 2.9452, time 234.75ms, mfu 2.19%\n",
            "iter 710: loss 3.0116, time 236.04ms, mfu 2.19%\n",
            "iter 720: loss 2.7912, time 235.05ms, mfu 2.19%\n",
            "iter 730: loss 2.9559, time 234.64ms, mfu 2.19%\n",
            "iter 740: loss 2.8857, time 234.12ms, mfu 2.19%\n",
            "step 750: train loss 2.6812, val loss 5.4415\n",
            "saving checkpoint to out\n",
            "iter 750: loss 2.8263, time 24852.53ms, mfu 1.98%\n",
            "iter 760: loss 2.8438, time 229.05ms, mfu 2.00%\n",
            "iter 770: loss 2.7516, time 232.34ms, mfu 2.03%\n",
            "iter 780: loss 2.7625, time 232.11ms, mfu 2.04%\n",
            "iter 790: loss 2.6730, time 234.37ms, mfu 2.06%\n",
            "iter 800: loss 2.9702, time 232.19ms, mfu 2.08%\n",
            "iter 810: loss 2.7751, time 231.05ms, mfu 2.09%\n",
            "iter 820: loss 2.7491, time 229.98ms, mfu 2.11%\n",
            "iter 830: loss 2.6051, time 231.93ms, mfu 2.12%\n",
            "iter 840: loss 2.6810, time 230.73ms, mfu 2.13%\n",
            "iter 850: loss 2.8753, time 230.28ms, mfu 2.14%\n",
            "iter 860: loss 2.6481, time 233.60ms, mfu 2.15%\n",
            "iter 870: loss 2.5200, time 229.72ms, mfu 2.16%\n",
            "iter 880: loss 2.5384, time 231.37ms, mfu 2.16%\n",
            "iter 890: loss 2.5141, time 232.22ms, mfu 2.17%\n",
            "iter 900: loss 2.4133, time 231.86ms, mfu 2.17%\n",
            "iter 910: loss 2.5475, time 230.53ms, mfu 2.18%\n",
            "iter 920: loss 2.4808, time 231.20ms, mfu 2.19%\n",
            "iter 930: loss 2.3649, time 230.06ms, mfu 2.19%\n",
            "iter 940: loss 2.5121, time 232.35ms, mfu 2.19%\n",
            "iter 950: loss 2.4029, time 232.90ms, mfu 2.20%\n",
            "iter 960: loss 2.1046, time 234.54ms, mfu 2.20%\n",
            "iter 970: loss 2.2022, time 233.68ms, mfu 2.20%\n",
            "iter 980: loss 2.2747, time 233.09ms, mfu 2.20%\n",
            "iter 990: loss 2.1398, time 231.77ms, mfu 2.20%\n",
            "step 1000: train loss 1.9292, val loss 5.8802\n",
            "saving checkpoint to out\n",
            "iter 1000: loss 2.4285, time 24852.59ms, mfu 1.98%\n",
            "iter 1010: loss 2.1970, time 232.07ms, mfu 2.01%\n",
            "iter 1020: loss 2.2799, time 233.15ms, mfu 2.03%\n",
            "iter 1030: loss 2.3204, time 232.54ms, mfu 2.05%\n",
            "iter 1040: loss 1.9771, time 231.27ms, mfu 2.06%\n",
            "iter 1050: loss 1.9858, time 229.75ms, mfu 2.08%\n",
            "iter 1060: loss 2.0979, time 231.81ms, mfu 2.10%\n",
            "iter 1070: loss 1.9561, time 230.20ms, mfu 2.11%\n",
            "iter 1080: loss 2.1586, time 229.97ms, mfu 2.12%\n",
            "iter 1090: loss 1.8059, time 229.20ms, mfu 2.14%\n",
            "iter 1100: loss 2.0124, time 228.35ms, mfu 2.15%\n",
            "iter 1110: loss 1.9815, time 229.77ms, mfu 2.16%\n",
            "iter 1120: loss 1.9480, time 230.11ms, mfu 2.17%\n",
            "iter 1130: loss 1.8215, time 228.78ms, mfu 2.17%\n",
            "iter 1140: loss 1.8424, time 226.50ms, mfu 2.18%\n",
            "iter 1150: loss 1.8913, time 234.87ms, mfu 2.19%\n",
            "iter 1160: loss 1.9752, time 230.09ms, mfu 2.19%\n",
            "iter 1170: loss 1.8974, time 227.99ms, mfu 2.20%\n",
            "iter 1180: loss 1.7075, time 231.37ms, mfu 2.20%\n",
            "iter 1190: loss 1.8969, time 230.58ms, mfu 2.20%\n",
            "iter 1200: loss 1.8509, time 232.21ms, mfu 2.21%\n",
            "iter 1210: loss 1.9154, time 230.62ms, mfu 2.21%\n",
            "iter 1220: loss 1.7920, time 230.30ms, mfu 2.21%\n",
            "iter 1230: loss 1.7602, time 232.71ms, mfu 2.21%\n",
            "iter 1240: loss 1.6863, time 228.14ms, mfu 2.22%\n",
            "step 1250: train loss 1.3093, val loss 6.3735\n",
            "saving checkpoint to out\n",
            "iter 1250: loss 1.6703, time 24784.75ms, mfu 2.00%\n",
            "iter 1260: loss 1.6628, time 231.81ms, mfu 2.02%\n",
            "iter 1270: loss 1.7915, time 230.54ms, mfu 2.04%\n",
            "iter 1280: loss 1.5944, time 231.64ms, mfu 2.06%\n",
            "iter 1290: loss 1.5863, time 231.19ms, mfu 2.08%\n",
            "iter 1300: loss 1.6887, time 232.68ms, mfu 2.09%\n",
            "iter 1310: loss 1.6537, time 230.59ms, mfu 2.11%\n",
            "iter 1320: loss 1.5237, time 230.79ms, mfu 2.12%\n",
            "iter 1330: loss 1.4792, time 231.77ms, mfu 2.13%\n",
            "iter 1340: loss 1.4981, time 234.08ms, mfu 2.14%\n",
            "iter 1350: loss 1.5742, time 233.38ms, mfu 2.14%\n",
            "iter 1360: loss 1.6187, time 231.47ms, mfu 2.15%\n",
            "iter 1370: loss 1.4849, time 232.13ms, mfu 2.16%\n",
            "iter 1380: loss 1.5763, time 231.46ms, mfu 2.17%\n",
            "iter 1390: loss 1.5379, time 231.78ms, mfu 2.17%\n",
            "iter 1400: loss 1.4362, time 231.42ms, mfu 2.18%\n",
            "iter 1410: loss 1.4426, time 233.13ms, mfu 2.18%\n",
            "iter 1420: loss 1.4915, time 231.62ms, mfu 2.18%\n",
            "iter 1430: loss 1.4665, time 228.49ms, mfu 2.19%\n",
            "iter 1440: loss 1.3877, time 228.91ms, mfu 2.20%\n",
            "iter 1450: loss 1.5317, time 229.39ms, mfu 2.20%\n",
            "iter 1460: loss 1.3057, time 229.46ms, mfu 2.21%\n",
            "iter 1470: loss 1.4275, time 231.26ms, mfu 2.21%\n",
            "iter 1480: loss 1.3248, time 232.20ms, mfu 2.21%\n",
            "iter 1490: loss 1.4422, time 231.44ms, mfu 2.21%\n",
            "step 1500: train loss 0.9176, val loss 6.8480\n",
            "saving checkpoint to out\n",
            "iter 1500: loss 1.3575, time 24847.39ms, mfu 1.99%\n",
            "iter 1510: loss 1.3301, time 233.29ms, mfu 2.01%\n",
            "iter 1520: loss 1.4120, time 228.83ms, mfu 2.04%\n",
            "iter 1530: loss 1.2316, time 232.58ms, mfu 2.06%\n",
            "iter 1540: loss 1.2666, time 229.52ms, mfu 2.08%\n",
            "iter 1550: loss 1.2460, time 229.33ms, mfu 2.09%\n",
            "iter 1560: loss 1.2244, time 230.85ms, mfu 2.11%\n",
            "iter 1570: loss 1.2299, time 230.53ms, mfu 2.12%\n",
            "iter 1580: loss 1.2386, time 228.48ms, mfu 2.13%\n",
            "iter 1590: loss 1.1987, time 230.93ms, mfu 2.14%\n",
            "iter 1600: loss 1.2352, time 231.56ms, mfu 2.15%\n",
            "iter 1610: loss 1.2844, time 229.60ms, mfu 2.16%\n",
            "iter 1620: loss 1.2221, time 230.36ms, mfu 2.17%\n",
            "iter 1630: loss 1.2377, time 232.82ms, mfu 2.17%\n",
            "iter 1640: loss 1.1732, time 234.75ms, mfu 2.18%\n",
            "iter 1650: loss 1.1946, time 231.27ms, mfu 2.18%\n",
            "iter 1660: loss 1.1566, time 229.71ms, mfu 2.19%\n",
            "iter 1670: loss 1.1047, time 230.13ms, mfu 2.19%\n",
            "iter 1680: loss 1.1569, time 229.49ms, mfu 2.20%\n",
            "iter 1690: loss 1.2745, time 229.16ms, mfu 2.20%\n",
            "iter 1700: loss 1.1433, time 230.07ms, mfu 2.21%\n",
            "iter 1710: loss 1.0181, time 231.49ms, mfu 2.21%\n",
            "iter 1720: loss 1.0755, time 230.80ms, mfu 2.21%\n",
            "iter 1730: loss 1.1116, time 229.33ms, mfu 2.21%\n",
            "iter 1740: loss 1.0710, time 228.28ms, mfu 2.22%\n",
            "step 1750: train loss 0.6712, val loss 7.1778\n",
            "saving checkpoint to out\n",
            "iter 1750: loss 1.0603, time 24747.67ms, mfu 2.00%\n",
            "iter 1760: loss 1.0807, time 228.12ms, mfu 2.03%\n",
            "iter 1770: loss 1.0687, time 230.11ms, mfu 2.05%\n",
            "iter 1780: loss 1.0287, time 230.71ms, mfu 2.07%\n",
            "iter 1790: loss 1.1365, time 230.62ms, mfu 2.08%\n",
            "iter 1800: loss 1.1069, time 229.94ms, mfu 2.10%\n",
            "iter 1810: loss 0.9949, time 230.15ms, mfu 2.11%\n",
            "iter 1820: loss 1.0931, time 229.46ms, mfu 2.13%\n",
            "iter 1830: loss 0.9849, time 230.82ms, mfu 2.14%\n",
            "iter 1840: loss 1.0347, time 230.59ms, mfu 2.15%\n",
            "iter 1850: loss 1.0073, time 231.72ms, mfu 2.15%\n",
            "iter 1860: loss 1.0111, time 230.00ms, mfu 2.16%\n",
            "iter 1870: loss 0.9063, time 231.64ms, mfu 2.17%\n",
            "iter 1880: loss 1.0003, time 231.73ms, mfu 2.17%\n",
            "iter 1890: loss 0.9889, time 230.79ms, mfu 2.18%\n",
            "iter 1900: loss 0.9939, time 231.98ms, mfu 2.18%\n",
            "iter 1910: loss 0.9260, time 228.49ms, mfu 2.19%\n",
            "iter 1920: loss 0.9850, time 231.56ms, mfu 2.20%\n",
            "iter 1930: loss 0.9553, time 227.90ms, mfu 2.20%\n",
            "iter 1940: loss 0.9951, time 230.33ms, mfu 2.21%\n",
            "iter 1950: loss 0.9252, time 231.18ms, mfu 2.21%\n",
            "iter 1960: loss 0.9515, time 229.10ms, mfu 2.21%\n",
            "iter 1970: loss 0.9270, time 228.59ms, mfu 2.22%\n",
            "iter 1980: loss 0.9172, time 229.26ms, mfu 2.22%\n",
            "iter 1990: loss 0.9183, time 227.31ms, mfu 2.22%\n",
            "step 2000: train loss 0.5125, val loss 7.5443\n",
            "saving checkpoint to out\n",
            "iter 2000: loss 0.8939, time 24769.20ms, mfu 2.00%\n",
            "iter 2010: loss 0.9293, time 230.72ms, mfu 2.03%\n",
            "iter 2020: loss 0.8795, time 228.30ms, mfu 2.05%\n",
            "iter 2030: loss 0.8321, time 230.24ms, mfu 2.07%\n",
            "iter 2040: loss 0.8140, time 229.17ms, mfu 2.09%\n",
            "iter 2050: loss 0.8375, time 226.52ms, mfu 2.11%\n",
            "iter 2060: loss 0.8512, time 230.23ms, mfu 2.12%\n",
            "iter 2070: loss 0.7845, time 232.04ms, mfu 2.13%\n",
            "iter 2080: loss 0.8177, time 231.63ms, mfu 2.14%\n",
            "iter 2090: loss 0.8059, time 230.40ms, mfu 2.15%\n",
            "iter 2100: loss 0.8575, time 232.15ms, mfu 2.16%\n",
            "iter 2110: loss 0.8404, time 230.92ms, mfu 2.16%\n",
            "iter 2120: loss 0.8599, time 228.93ms, mfu 2.17%\n",
            "iter 2130: loss 0.8528, time 230.84ms, mfu 2.18%\n",
            "iter 2140: loss 0.8949, time 229.65ms, mfu 2.18%\n",
            "iter 2150: loss 0.7707, time 230.38ms, mfu 2.19%\n",
            "iter 2160: loss 0.7832, time 234.33ms, mfu 2.19%\n",
            "iter 2170: loss 0.8892, time 232.64ms, mfu 2.19%\n",
            "iter 2180: loss 0.8588, time 229.34ms, mfu 2.20%\n",
            "iter 2190: loss 0.8168, time 232.22ms, mfu 2.20%\n",
            "iter 2200: loss 0.7464, time 231.56ms, mfu 2.20%\n",
            "iter 2210: loss 0.7606, time 234.33ms, mfu 2.20%\n",
            "iter 2220: loss 0.7961, time 227.90ms, mfu 2.21%\n",
            "iter 2230: loss 0.7375, time 229.40ms, mfu 2.21%\n",
            "iter 2240: loss 0.7488, time 228.85ms, mfu 2.22%\n",
            "step 2250: train loss 0.3877, val loss 7.8554\n",
            "saving checkpoint to out\n",
            "iter 2250: loss 0.7356, time 24811.76ms, mfu 2.00%\n",
            "iter 2260: loss 0.7805, time 228.65ms, mfu 2.02%\n",
            "iter 2270: loss 0.7105, time 230.16ms, mfu 2.04%\n",
            "iter 2280: loss 0.6866, time 231.05ms, mfu 2.06%\n",
            "iter 2290: loss 0.7241, time 230.36ms, mfu 2.08%\n",
            "iter 2300: loss 0.7690, time 230.85ms, mfu 2.10%\n",
            "iter 2310: loss 0.6883, time 227.80ms, mfu 2.11%\n",
            "iter 2320: loss 0.7002, time 229.05ms, mfu 2.13%\n",
            "iter 2330: loss 0.7006, time 232.53ms, mfu 2.14%\n",
            "iter 2340: loss 0.7617, time 230.17ms, mfu 2.15%\n",
            "iter 2350: loss 0.7223, time 228.28ms, mfu 2.16%\n",
            "iter 2360: loss 0.7201, time 231.28ms, mfu 2.16%\n",
            "iter 2370: loss 0.6911, time 233.02ms, mfu 2.17%\n",
            "iter 2380: loss 0.6938, time 228.45ms, mfu 2.18%\n",
            "iter 2390: loss 0.7004, time 229.54ms, mfu 2.18%\n",
            "iter 2400: loss 0.6587, time 231.81ms, mfu 2.19%\n",
            "iter 2410: loss 0.7944, time 228.85ms, mfu 2.19%\n",
            "iter 2420: loss 0.6547, time 229.38ms, mfu 2.20%\n",
            "iter 2430: loss 0.6425, time 229.77ms, mfu 2.20%\n",
            "iter 2440: loss 0.6651, time 229.02ms, mfu 2.21%\n",
            "iter 2450: loss 0.6515, time 228.37ms, mfu 2.21%\n",
            "iter 2460: loss 0.5961, time 230.54ms, mfu 2.22%\n",
            "iter 2470: loss 0.6592, time 228.15ms, mfu 2.22%\n",
            "iter 2480: loss 0.6668, time 230.41ms, mfu 2.22%\n",
            "iter 2490: loss 0.6212, time 230.26ms, mfu 2.22%\n",
            "step 2500: train loss 0.3072, val loss 8.1621\n",
            "saving checkpoint to out\n",
            "iter 2500: loss 0.6427, time 24840.28ms, mfu 2.00%\n",
            "iter 2510: loss 0.6001, time 229.51ms, mfu 2.03%\n",
            "iter 2520: loss 0.6244, time 229.82ms, mfu 2.05%\n",
            "iter 2530: loss 0.5704, time 229.99ms, mfu 2.07%\n",
            "iter 2540: loss 0.6620, time 227.78ms, mfu 2.09%\n",
            "iter 2550: loss 0.5914, time 230.65ms, mfu 2.10%\n",
            "iter 2560: loss 0.5914, time 230.63ms, mfu 2.12%\n",
            "iter 2570: loss 0.6433, time 231.27ms, mfu 2.13%\n",
            "iter 2580: loss 0.6357, time 231.29ms, mfu 2.14%\n",
            "iter 2590: loss 0.5759, time 230.94ms, mfu 2.15%\n",
            "iter 2600: loss 0.5920, time 229.44ms, mfu 2.16%\n",
            "iter 2610: loss 0.5989, time 227.71ms, mfu 2.17%\n",
            "iter 2620: loss 0.5841, time 232.10ms, mfu 2.17%\n",
            "iter 2630: loss 0.5731, time 230.56ms, mfu 2.18%\n",
            "iter 2640: loss 0.5611, time 227.09ms, mfu 2.19%\n",
            "iter 2650: loss 0.5692, time 229.91ms, mfu 2.19%\n",
            "iter 2660: loss 0.5901, time 229.27ms, mfu 2.20%\n",
            "iter 2670: loss 0.5484, time 228.74ms, mfu 2.20%\n",
            "iter 2680: loss 0.6091, time 232.33ms, mfu 2.21%\n",
            "iter 2690: loss 0.5850, time 227.62ms, mfu 2.21%\n",
            "iter 2700: loss 0.5293, time 229.85ms, mfu 2.21%\n",
            "iter 2710: loss 0.5507, time 229.51ms, mfu 2.22%\n",
            "iter 2720: loss 0.5415, time 229.16ms, mfu 2.22%\n",
            "iter 2730: loss 0.5135, time 227.88ms, mfu 2.23%\n",
            "iter 2740: loss 0.5049, time 229.10ms, mfu 2.23%\n",
            "step 2750: train loss 0.2353, val loss 8.3615\n",
            "saving checkpoint to out\n",
            "iter 2750: loss 0.6007, time 24812.15ms, mfu 2.01%\n",
            "iter 2760: loss 0.5837, time 230.31ms, mfu 2.03%\n",
            "iter 2770: loss 0.4989, time 229.16ms, mfu 2.05%\n",
            "iter 2780: loss 0.5468, time 227.90ms, mfu 2.07%\n",
            "iter 2790: loss 0.4823, time 231.73ms, mfu 2.09%\n",
            "iter 2800: loss 0.4959, time 230.22ms, mfu 2.10%\n",
            "iter 2810: loss 0.5548, time 228.49ms, mfu 2.12%\n",
            "iter 2820: loss 0.4756, time 230.55ms, mfu 2.13%\n",
            "iter 2830: loss 0.4821, time 229.24ms, mfu 2.14%\n",
            "iter 2840: loss 0.5239, time 232.02ms, mfu 2.15%\n",
            "iter 2850: loss 0.4762, time 230.25ms, mfu 2.16%\n",
            "iter 2860: loss 0.4666, time 231.90ms, mfu 2.17%\n",
            "iter 2870: loss 0.4978, time 234.76ms, mfu 2.17%\n",
            "iter 2880: loss 0.4692, time 227.24ms, mfu 2.18%\n",
            "iter 2890: loss 0.4706, time 231.07ms, mfu 2.18%\n",
            "iter 2900: loss 0.4687, time 231.09ms, mfu 2.19%\n",
            "iter 2910: loss 0.4315, time 232.04ms, mfu 2.19%\n",
            "iter 2920: loss 0.4601, time 234.07ms, mfu 2.19%\n",
            "iter 2930: loss 0.5019, time 231.63ms, mfu 2.20%\n",
            "iter 2940: loss 0.4703, time 228.38ms, mfu 2.20%\n",
            "iter 2950: loss 0.4754, time 231.66ms, mfu 2.20%\n",
            "iter 2960: loss 0.4507, time 231.25ms, mfu 2.21%\n",
            "iter 2970: loss 0.4509, time 229.45ms, mfu 2.21%\n",
            "iter 2980: loss 0.4507, time 231.77ms, mfu 2.21%\n",
            "iter 2990: loss 0.4535, time 228.97ms, mfu 2.22%\n",
            "step 3000: train loss 0.1891, val loss 8.5240\n",
            "saving checkpoint to out\n",
            "iter 3000: loss 0.4587, time 24735.67ms, mfu 2.00%\n",
            "iter 3010: loss 0.4707, time 230.77ms, mfu 2.02%\n",
            "iter 3020: loss 0.4379, time 230.16ms, mfu 2.04%\n",
            "iter 3030: loss 0.4435, time 228.78ms, mfu 2.06%\n",
            "iter 3040: loss 0.4607, time 229.64ms, mfu 2.08%\n",
            "iter 3050: loss 0.4376, time 229.35ms, mfu 2.10%\n",
            "iter 3060: loss 0.4256, time 228.21ms, mfu 2.11%\n",
            "iter 3070: loss 0.4262, time 228.50ms, mfu 2.13%\n",
            "iter 3080: loss 0.4121, time 230.64ms, mfu 2.14%\n",
            "iter 3090: loss 0.3938, time 229.06ms, mfu 2.15%\n",
            "iter 3100: loss 0.4420, time 227.67ms, mfu 2.16%\n",
            "iter 3110: loss 0.4387, time 228.48ms, mfu 2.17%\n",
            "iter 3120: loss 0.4165, time 234.06ms, mfu 2.17%\n",
            "iter 3130: loss 0.4517, time 230.71ms, mfu 2.18%\n",
            "iter 3140: loss 0.4595, time 229.94ms, mfu 2.19%\n",
            "iter 3150: loss 0.4306, time 229.64ms, mfu 2.19%\n",
            "iter 3160: loss 0.4120, time 229.40ms, mfu 2.20%\n",
            "iter 3170: loss 0.4339, time 230.36ms, mfu 2.20%\n",
            "iter 3180: loss 0.3841, time 233.04ms, mfu 2.20%\n",
            "iter 3190: loss 0.3869, time 230.16ms, mfu 2.21%\n",
            "iter 3200: loss 0.4317, time 230.09ms, mfu 2.21%\n",
            "iter 3210: loss 0.4310, time 227.22ms, mfu 2.22%\n",
            "iter 3220: loss 0.3458, time 231.16ms, mfu 2.22%\n",
            "iter 3230: loss 0.4221, time 226.53ms, mfu 2.22%\n",
            "iter 3240: loss 0.3862, time 228.80ms, mfu 2.23%\n",
            "step 3250: train loss 0.1511, val loss 8.6501\n",
            "saving checkpoint to out\n",
            "iter 3250: loss 0.4208, time 24751.96ms, mfu 2.01%\n",
            "iter 3260: loss 0.3558, time 227.84ms, mfu 2.03%\n",
            "iter 3270: loss 0.3524, time 229.02ms, mfu 2.05%\n",
            "iter 3280: loss 0.3811, time 229.16ms, mfu 2.07%\n",
            "iter 3290: loss 0.3764, time 230.48ms, mfu 2.09%\n",
            "iter 3300: loss 0.3716, time 227.90ms, mfu 2.11%\n",
            "iter 3310: loss 0.3918, time 232.51ms, mfu 2.12%\n",
            "iter 3320: loss 0.3704, time 228.24ms, mfu 2.13%\n",
            "iter 3330: loss 0.3644, time 227.87ms, mfu 2.14%\n",
            "iter 3340: loss 0.3948, time 231.03ms, mfu 2.15%\n",
            "iter 3350: loss 0.3757, time 228.73ms, mfu 2.16%\n",
            "iter 3360: loss 0.3910, time 230.12ms, mfu 2.17%\n",
            "iter 3370: loss 0.3727, time 227.97ms, mfu 2.18%\n",
            "iter 3380: loss 0.3764, time 228.79ms, mfu 2.19%\n",
            "iter 3390: loss 0.3530, time 228.93ms, mfu 2.19%\n",
            "iter 3400: loss 0.3450, time 228.84ms, mfu 2.20%\n",
            "iter 3410: loss 0.3762, time 228.92ms, mfu 2.20%\n",
            "iter 3420: loss 0.3506, time 231.59ms, mfu 2.21%\n",
            "iter 3430: loss 0.3656, time 229.52ms, mfu 2.21%\n",
            "iter 3440: loss 0.3371, time 231.16ms, mfu 2.21%\n",
            "iter 3450: loss 0.3372, time 230.96ms, mfu 2.21%\n",
            "iter 3460: loss 0.3567, time 227.75ms, mfu 2.22%\n",
            "iter 3470: loss 0.3169, time 230.45ms, mfu 2.22%\n",
            "iter 3480: loss 0.3158, time 231.23ms, mfu 2.22%\n",
            "iter 3490: loss 0.3532, time 230.65ms, mfu 2.22%\n",
            "step 3500: train loss 0.1305, val loss 8.8202\n",
            "saving checkpoint to out\n",
            "iter 3500: loss 0.3097, time 24723.57ms, mfu 2.00%\n",
            "iter 3510: loss 0.3096, time 228.50ms, mfu 2.03%\n",
            "iter 3520: loss 0.3673, time 227.90ms, mfu 2.05%\n",
            "iter 3530: loss 0.2967, time 227.98ms, mfu 2.07%\n",
            "iter 3540: loss 0.3187, time 228.06ms, mfu 2.09%\n",
            "iter 3550: loss 0.3018, time 231.02ms, mfu 2.11%\n",
            "iter 3560: loss 0.3399, time 230.73ms, mfu 2.12%\n",
            "iter 3570: loss 0.3227, time 229.33ms, mfu 2.13%\n",
            "iter 3580: loss 0.3408, time 229.18ms, mfu 2.14%\n",
            "iter 3590: loss 0.3043, time 228.33ms, mfu 2.15%\n",
            "iter 3600: loss 0.3273, time 228.46ms, mfu 2.16%\n",
            "iter 3610: loss 0.3056, time 231.81ms, mfu 2.17%\n",
            "iter 3620: loss 0.3172, time 229.80ms, mfu 2.18%\n",
            "iter 3630: loss 0.3076, time 231.76ms, mfu 2.18%\n",
            "iter 3640: loss 0.3074, time 230.07ms, mfu 2.19%\n",
            "iter 3650: loss 0.2949, time 228.41ms, mfu 2.19%\n",
            "iter 3660: loss 0.2826, time 229.04ms, mfu 2.20%\n",
            "iter 3670: loss 0.2854, time 228.72ms, mfu 2.21%\n",
            "iter 3680: loss 0.3356, time 229.19ms, mfu 2.21%\n",
            "iter 3690: loss 0.3103, time 231.14ms, mfu 2.21%\n",
            "iter 3700: loss 0.2959, time 227.41ms, mfu 2.22%\n",
            "iter 3710: loss 0.2894, time 230.29ms, mfu 2.22%\n",
            "iter 3720: loss 0.2733, time 231.10ms, mfu 2.22%\n",
            "iter 3730: loss 0.2871, time 229.52ms, mfu 2.22%\n",
            "iter 3740: loss 0.2888, time 230.35ms, mfu 2.22%\n",
            "step 3750: train loss 0.1090, val loss 8.8834\n",
            "saving checkpoint to out\n",
            "iter 3750: loss 0.3080, time 24720.77ms, mfu 2.00%\n",
            "iter 3760: loss 0.3099, time 229.61ms, mfu 2.03%\n",
            "iter 3770: loss 0.2862, time 229.47ms, mfu 2.05%\n",
            "iter 3780: loss 0.2885, time 230.98ms, mfu 2.07%\n",
            "iter 3790: loss 0.2897, time 228.91ms, mfu 2.09%\n",
            "iter 3800: loss 0.2891, time 228.73ms, mfu 2.10%\n",
            "iter 3810: loss 0.2943, time 229.08ms, mfu 2.12%\n",
            "iter 3820: loss 0.2721, time 229.52ms, mfu 2.13%\n",
            "iter 3830: loss 0.2835, time 229.28ms, mfu 2.14%\n",
            "iter 3840: loss 0.2636, time 229.96ms, mfu 2.15%\n",
            "iter 3850: loss 0.2893, time 228.08ms, mfu 2.16%\n",
            "iter 3860: loss 0.2501, time 228.51ms, mfu 2.17%\n",
            "iter 3870: loss 0.2664, time 230.74ms, mfu 2.18%\n",
            "iter 3880: loss 0.2743, time 232.61ms, mfu 2.18%\n",
            "iter 3890: loss 0.2714, time 230.54ms, mfu 2.19%\n",
            "iter 3900: loss 0.2786, time 231.35ms, mfu 2.19%\n",
            "iter 3910: loss 0.2642, time 231.03ms, mfu 2.20%\n",
            "iter 3920: loss 0.2634, time 229.90ms, mfu 2.20%\n",
            "iter 3930: loss 0.2592, time 229.98ms, mfu 2.20%\n",
            "iter 3940: loss 0.2828, time 229.11ms, mfu 2.21%\n",
            "iter 3950: loss 0.2748, time 229.70ms, mfu 2.21%\n",
            "iter 3960: loss 0.2578, time 231.80ms, mfu 2.21%\n",
            "iter 3970: loss 0.2662, time 228.90ms, mfu 2.22%\n",
            "iter 3980: loss 0.2277, time 230.93ms, mfu 2.22%\n",
            "iter 3990: loss 0.2279, time 229.41ms, mfu 2.22%\n",
            "step 4000: train loss 0.0989, val loss 8.9982\n",
            "saving checkpoint to out\n",
            "iter 4000: loss 0.2663, time 24711.45ms, mfu 2.00%\n",
            "iter 4010: loss 0.2400, time 227.41ms, mfu 2.03%\n",
            "iter 4020: loss 0.2725, time 231.57ms, mfu 2.05%\n",
            "iter 4030: loss 0.2569, time 227.52ms, mfu 2.07%\n",
            "iter 4040: loss 0.2438, time 228.72ms, mfu 2.09%\n",
            "iter 4050: loss 0.2561, time 229.36ms, mfu 2.10%\n",
            "iter 4060: loss 0.2653, time 228.76ms, mfu 2.12%\n",
            "iter 4070: loss 0.2524, time 228.47ms, mfu 2.13%\n",
            "iter 4080: loss 0.2561, time 230.47ms, mfu 2.14%\n",
            "iter 4090: loss 0.2667, time 229.96ms, mfu 2.15%\n",
            "iter 4100: loss 0.2483, time 229.01ms, mfu 2.16%\n",
            "iter 4110: loss 0.2341, time 232.14ms, mfu 2.17%\n",
            "iter 4120: loss 0.2609, time 231.28ms, mfu 2.17%\n",
            "iter 4130: loss 0.2539, time 228.87ms, mfu 2.18%\n",
            "iter 4140: loss 0.2641, time 231.77ms, mfu 2.19%\n",
            "iter 4150: loss 0.2517, time 228.58ms, mfu 2.19%\n",
            "iter 4160: loss 0.2556, time 230.73ms, mfu 2.20%\n",
            "iter 4170: loss 0.2262, time 232.12ms, mfu 2.20%\n",
            "iter 4180: loss 0.2453, time 232.26ms, mfu 2.20%\n",
            "iter 4190: loss 0.2430, time 229.78ms, mfu 2.21%\n",
            "iter 4200: loss 0.2799, time 229.62ms, mfu 2.21%\n",
            "iter 4210: loss 0.2633, time 229.36ms, mfu 2.21%\n",
            "iter 4220: loss 0.2444, time 231.65ms, mfu 2.21%\n",
            "iter 4230: loss 0.2299, time 229.56ms, mfu 2.22%\n",
            "iter 4240: loss 0.2403, time 231.79ms, mfu 2.22%\n",
            "step 4250: train loss 0.0894, val loss 9.0656\n",
            "saving checkpoint to out\n",
            "iter 4250: loss 0.2434, time 24738.25ms, mfu 2.00%\n",
            "iter 4260: loss 0.2332, time 228.67ms, mfu 2.02%\n",
            "iter 4270: loss 0.2436, time 228.08ms, mfu 2.05%\n",
            "iter 4280: loss 0.2525, time 229.07ms, mfu 2.07%\n",
            "iter 4290: loss 0.2172, time 227.58ms, mfu 2.09%\n",
            "iter 4300: loss 0.2538, time 230.46ms, mfu 2.10%\n",
            "iter 4310: loss 0.2181, time 229.25ms, mfu 2.12%\n",
            "iter 4320: loss 0.2303, time 229.30ms, mfu 2.13%\n",
            "iter 4330: loss 0.2339, time 228.39ms, mfu 2.14%\n",
            "iter 4340: loss 0.2516, time 231.19ms, mfu 2.15%\n",
            "iter 4350: loss 0.2269, time 229.63ms, mfu 2.16%\n",
            "iter 4360: loss 0.2404, time 229.26ms, mfu 2.17%\n",
            "iter 4370: loss 0.2323, time 230.88ms, mfu 2.18%\n",
            "iter 4380: loss 0.2117, time 232.76ms, mfu 2.18%\n",
            "iter 4390: loss 0.2179, time 231.09ms, mfu 2.18%\n",
            "iter 4400: loss 0.2000, time 231.19ms, mfu 2.19%\n",
            "iter 4410: loss 0.2131, time 231.02ms, mfu 2.19%\n",
            "iter 4420: loss 0.2204, time 230.15ms, mfu 2.20%\n",
            "iter 4430: loss 0.2337, time 227.77ms, mfu 2.20%\n",
            "iter 4440: loss 0.2139, time 228.78ms, mfu 2.21%\n",
            "iter 4450: loss 0.2177, time 230.11ms, mfu 2.21%\n",
            "iter 4460: loss 0.2102, time 230.14ms, mfu 2.22%\n",
            "iter 4470: loss 0.2361, time 229.89ms, mfu 2.22%\n",
            "iter 4480: loss 0.2038, time 227.13ms, mfu 2.22%\n",
            "iter 4490: loss 0.2195, time 232.15ms, mfu 2.22%\n",
            "step 4500: train loss 0.0821, val loss 9.0821\n",
            "saving checkpoint to out\n",
            "iter 4500: loss 0.2218, time 24643.98ms, mfu 2.00%\n",
            "iter 4510: loss 0.2335, time 227.47ms, mfu 2.03%\n",
            "iter 4520: loss 0.2375, time 230.46ms, mfu 2.05%\n",
            "iter 4530: loss 0.2079, time 229.65ms, mfu 2.07%\n",
            "iter 4540: loss 0.2053, time 228.16ms, mfu 2.09%\n",
            "iter 4550: loss 0.2038, time 229.45ms, mfu 2.10%\n",
            "iter 4560: loss 0.2164, time 226.86ms, mfu 2.12%\n",
            "iter 4570: loss 0.2077, time 229.93ms, mfu 2.13%\n",
            "iter 4580: loss 0.2238, time 231.64ms, mfu 2.14%\n",
            "iter 4590: loss 0.2236, time 230.28ms, mfu 2.15%\n",
            "iter 4600: loss 0.2262, time 229.20ms, mfu 2.16%\n",
            "iter 4610: loss 0.2108, time 229.48ms, mfu 2.17%\n",
            "iter 4620: loss 0.1985, time 227.77ms, mfu 2.18%\n",
            "iter 4630: loss 0.2240, time 228.25ms, mfu 2.19%\n",
            "iter 4640: loss 0.1884, time 229.08ms, mfu 2.19%\n",
            "iter 4650: loss 0.2090, time 231.16ms, mfu 2.20%\n",
            "iter 4660: loss 0.2086, time 230.19ms, mfu 2.20%\n",
            "iter 4670: loss 0.2240, time 228.97ms, mfu 2.21%\n",
            "iter 4680: loss 0.2229, time 231.27ms, mfu 2.21%\n",
            "iter 4690: loss 0.2119, time 230.19ms, mfu 2.21%\n",
            "iter 4700: loss 0.1934, time 227.17ms, mfu 2.22%\n",
            "iter 4710: loss 0.1984, time 229.87ms, mfu 2.22%\n",
            "iter 4720: loss 0.1902, time 230.88ms, mfu 2.22%\n",
            "iter 4730: loss 0.2230, time 230.87ms, mfu 2.22%\n",
            "iter 4740: loss 0.2279, time 229.72ms, mfu 2.22%\n",
            "step 4750: train loss 0.0790, val loss 9.2001\n",
            "saving checkpoint to out\n",
            "iter 4750: loss 0.2038, time 24783.50ms, mfu 2.00%\n",
            "iter 4760: loss 0.2244, time 227.14ms, mfu 2.03%\n",
            "iter 4770: loss 0.2030, time 232.27ms, mfu 2.05%\n",
            "iter 4780: loss 0.1921, time 226.99ms, mfu 2.07%\n",
            "iter 4790: loss 0.1858, time 230.39ms, mfu 2.09%\n",
            "iter 4800: loss 0.1975, time 230.00ms, mfu 2.10%\n",
            "iter 4810: loss 0.1985, time 228.39ms, mfu 2.12%\n",
            "iter 4820: loss 0.2005, time 230.05ms, mfu 2.13%\n",
            "iter 4830: loss 0.1978, time 229.04ms, mfu 2.14%\n",
            "iter 4840: loss 0.1913, time 231.41ms, mfu 2.15%\n",
            "iter 4850: loss 0.1802, time 230.95ms, mfu 2.16%\n",
            "iter 4860: loss 0.2114, time 229.14ms, mfu 2.17%\n",
            "iter 4870: loss 0.1985, time 231.24ms, mfu 2.17%\n",
            "iter 4880: loss 0.1781, time 231.14ms, mfu 2.18%\n",
            "iter 4890: loss 0.2061, time 230.95ms, mfu 2.19%\n",
            "iter 4900: loss 0.1835, time 230.38ms, mfu 2.19%\n",
            "iter 4910: loss 0.2032, time 230.03ms, mfu 2.20%\n",
            "iter 4920: loss 0.2079, time 231.20ms, mfu 2.20%\n",
            "iter 4930: loss 0.1873, time 231.04ms, mfu 2.20%\n",
            "iter 4940: loss 0.1974, time 228.50ms, mfu 2.21%\n",
            "iter 4950: loss 0.1962, time 231.54ms, mfu 2.21%\n",
            "iter 4960: loss 0.1922, time 229.74ms, mfu 2.21%\n",
            "iter 4970: loss 0.1862, time 229.71ms, mfu 2.22%\n",
            "iter 4980: loss 0.2124, time 229.03ms, mfu 2.22%\n",
            "iter 4990: loss 0.1881, time 229.53ms, mfu 2.22%\n",
            "step 5000: train loss 0.0758, val loss 9.2380\n",
            "saving checkpoint to out\n",
            "iter 5000: loss 0.1720, time 24756.54ms, mfu 2.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Outputs with our New Model\n",
        "\n",
        "Now we can leverage the `sample.py` file to generate outputs from our model!"
      ],
      "metadata": {
        "id": "L2J5JlRxFJOM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation Set Up and Model Loading"
      ],
      "metadata": {
        "id": "eo_QP1ITFfX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "import torch\n",
        "import tiktoken\n",
        "from model import GPTConfig, GPT\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
        "out_dir = 'out' # ignored if init_from is not 'resume'\n",
        "start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
        "num_samples = 10 # number of samples to draw\n",
        "max_new_tokens = 500 # number of tokens generated in each sample\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "seed = 1337\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
        "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
      ],
      "metadata": {
        "id": "-vftqU9LheEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "if init_from == 'resume':\n",
        "    # init from a model saved in a specific directory\n",
        "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
        "    model = GPT(gptconf)\n",
        "    state_dict = checkpoint['model']\n",
        "    unwanted_prefix = '_orig_mod.'\n",
        "    for k,v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "    model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQRB3j7iiNkl",
        "outputId": "a2046037-51b6-4187-f49a-52e8861b901b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 29.55M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "model.to(device)\n",
        "if compile:\n",
        "    model = torch.compile(model) # requires PyTorch 2.0 (optional)"
      ],
      "metadata": {
        "id": "N1YAy8DriVZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc = tokenizer\n",
        "encode = lambda s: enc.encode(s)\n",
        "decode = lambda l: enc.decode(l)"
      ],
      "metadata": {
        "id": "KoB-5ZuLicAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation!"
      ],
      "metadata": {
        "id": "mkTQ9wo7FjYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encode the beginning of the prompt\n",
        "if start.startswith('FILE:'):\n",
        "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
        "        start = f.read()\n",
        "start_ids = encode(start)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "# run generation\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print(decode(y[0].tolist()))\n",
        "            print('---------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmTcaHCjii5l",
        "outputId": "47f66833-fc35-495d-a3c0-7cff16718b9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "To the crown me in this golden crown,\n",
            "Command those few, revenues and moveables\n",
            "Whereof the last I did stand possess'd.\n",
            "\n",
            "KING RICHARD II:\n",
            "How well thou tell'st thou see that,\n",
            "Give me the king of Lancaster and my care?\n",
            "Dear brother, and my inward soul:\n",
            "Which finds Edward is lost for that,\n",
            "As Harry Duke of York, and grief.\n",
            "\n",
            "KING RICHARD II:\n",
            "But not the one:\n",
            "My lord, that I am;\n",
            "I for I not king, and all your comfort;\n",
            "With nothing had not been as for.\n",
            "\n",
            "Since I was? I am a dear;\n",
            "\n",
            "BUSHY:\n",
            "'Tis nothing but subjects,\n",
            "As little joy that I do not so my love.\n",
            "\n",
            "QUEEN:\n",
            "'Tis nothing less: conceit is still derived\n",
            "From some forefather grief; mine is not so,\n",
            "For nothing had begot my something grief;\n",
            "Or something hath the nothing that I grieve:\n",
            "'Tis in reversion that I do possess;\n",
            "But what it is, that is not yet known; what\n",
            "I cannot name; 'tis nameless woe, I wot.\n",
            "\n",
            "GREEN:\n",
            "God save your majesty! and well met, gentlemen:\n",
            "I hope the king is not yet shipp'd for Ireland.\n",
            "\n",
            "QUEEN:\n",
            "Why hopest thou so? 'tis better hope he is;\n",
            "For his designs crave haste, his haste good hope:\n",
            "Then wherefore dost thou hope he is not shipp'd?\n",
            "\n",
            "GREEN:\n",
            "That he, our hope, might have retired his power,\n",
            "And driven into despair an enemy's hope,\n",
            "Who strongly hath set footing in this land:\n",
            "The banish'd Bolingbroke repeals himself,\n",
            "And with uplifted arms is safe arrived\n",
            "At Ravenspurgh.\n",
            "\n",
            "QUEEN:\n",
            "Now God in heaven forbid!\n",
            "\n",
            "GREEN:\n",
            "Ah, madam, 'tis too true: and that is worse,\n",
            "The Lord Northumberland, his son young Henry Percy,\n",
            "The Lords of Ross, Beaumond, and Willoughby,\n",
            "With all their powerful friends, are fled to him.\n",
            "\n",
            "BUSHY:\n",
            "Why have you not proclaim'd Northumberland\n",
            "And all the rest revolted faction traitors?\n",
            "\n",
            "GREEN:\n",
            "We have: whereupon the Earl of Worcester\n",
            "---------------\n",
            "\n",
            "JULIET:\n",
            "Yea, wherefore?\n",
            "\n",
            "Nurse, I am here.\n",
            "\n",
            "JULIET:\n",
            "Well, come, thou hast comforted me marvellous much.\n",
            "Go in: and tell my lady I am gone,\n",
            "Having displeased my father, to Laurence' cell,\n",
            "To make confession and to be absolved.\n",
            "\n",
            "Nurse:\n",
            "Marry, I will; and this is wisely done.\n",
            "\n",
            "JULIET:\n",
            "Ancient damnation! O most wicked fiend!\n",
            "Is it more sin to wish me thus forsworn,\n",
            "Or to dispraise my lord with that same tongue\n",
            "Which she hath praised him with above compare\n",
            "So many thousand times? Go, counsellor;\n",
            "Thou and my bosom henceforth shall be twain.\n",
            "I'll to the friar, to know his remedy:\n",
            "If all else fail, myself have power to die.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "On Thursday, sir? the time is very short.\n",
            "\n",
            "PARIS:\n",
            "Of I am nothing slow to slack his haste.\n",
            "\n",
            "JULIET:\n",
            "You say you do not know the lady's mind:\n",
            "Uneven is the course, I like it not.\n",
            "\n",
            "PARIS:\n",
            "Immoderately she weeps for Tybalt's death,\n",
            "And therefore have I little talk'd of love;\n",
            "For Venus smiles not in a house of tears.\n",
            "Now, sir, her father counts it dangerous\n",
            "That she doth give her sorrow so much sway,\n",
            "And in his wisdom hastes our marriage,\n",
            "To stop the inundation of her tears;\n",
            "Which, too much minded by herself alone,\n",
            "May be put from her by society:\n",
            "Now do you know the reason of this haste.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "PARIS:\n",
            "Happily met, my lady and my wife!\n",
            "\n",
            "JULIET:\n",
            "That may be, sir, when I may be a wife.\n",
            "\n",
            "PARIS:\n",
            "That may be must be must be must be, on Thursday next.\n",
            "\n",
            "JULIET:\n",
            "What must be shall be shall be.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "That's a certain text.\n",
            "\n",
            "PARIS:\n",
            "Come you to make confession to this father?\n",
            "\n",
            "JULIET:\n",
            "To answer that, I should confess to you.\n",
            "\n",
            "PARIS:\n",
            "Do not\n",
            "---------------\n",
            "\n",
            "ISABELLA:\n",
            "The image of medicine\n",
            "But only he so seek.\n",
            "\n",
            "CLAUDIO:\n",
            "ISABELLA:\n",
            "O, but yet in heaven,\n",
            "I am the sin you consenting to't,\n",
            "Would bark your honour from that trunk you bear,\n",
            "And leave you naked.\n",
            "\n",
            "CLAUDIO:\n",
            "Let me know the point.\n",
            "\n",
            "ISABELLA:\n",
            "O, I do fear thee, Claudio; and I quake,\n",
            "Lest thou a feverous life shouldst entertain,\n",
            "And six or seven winters more respect\n",
            "Than a perpetual honour. Darest thou die?\n",
            "The sense of death is most in apprehension;\n",
            "And the poor beetle, that we tread upon,\n",
            "In corporal sufferance finds a pang as great\n",
            "As when a giant dies.\n",
            "\n",
            "CLAUDIO:\n",
            "Why give you me this shame?\n",
            "Think you I can a resolution fetch\n",
            "From flowery tenderness? If I must die,\n",
            "I will encounter darkness as a bride,\n",
            "And hug it in mine arms.\n",
            "\n",
            "ISABELLA:\n",
            "There spake my brother; there my father's grave\n",
            "Did utter forth a voice. Yes, thou must die:\n",
            "Thou art too noble to conserve a life\n",
            "In base appliances. This outward-sainted deputy,\n",
            "Whose settled visage and deliberate word\n",
            "Nips youth i' the head and follies doth emmew\n",
            "As falcon doth the fowl, is yet a devil\n",
            "His filth within being cast, he would appear\n",
            "A pond as deep as deep as hell.\n",
            "\n",
            "CLAUDIO:\n",
            "The prenzie Angelo!\n",
            "\n",
            "ISABELLA:\n",
            "O, 'tis the cunning livery of hell,\n",
            "The damned'st body to invest and cover\n",
            "In prenzie guards! Dost thou think, Claudio?\n",
            "If I would yield him my virginity,\n",
            "Thou mightst be freed.\n",
            "\n",
            "CLAUDIO:\n",
            "O heavens! it cannot be. This night's the time\n",
            "That I should do what I abhor to name,\n",
            "Or else thou diest to-morrow.\n",
            "\n",
            "ISABELLA:\n",
            "Thou shalt not do't.\n",
            "\n",
            "CLAUDIO:\n",
            "O, were it but my life,\n",
            "I'ld throw it down for your deliverance\n",
            "As frankly as a pin.\n",
            "\n",
            "ISABELLA:\n",
            "Thanks, dear Isabel.\n",
            "\n",
            "CLAUDIO:\n",
            "Be ready, dear Isabel.\n",
            "\n",
            "---------------\n",
            "\n",
            "\n",
            "KING RICHARD III:\n",
            "I did love her fair soul.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "But she, your mother.\n",
            "\n",
            "KING RICHARD III:\n",
            "What, I learn of bleeding- bleeding-hearts; thereon engrave\n",
            "Edward and York; then haply she will weep:\n",
            "Therefore present to her--as sometime Margaret\n",
            "Did to thy father, steep'd in Rutland's blood,--\n",
            "A handkerchief; which, say to her, did drain\n",
            "The purple sap from her sweet brother's body\n",
            "And bid her dry her dry her dry her weeping eyes therewith.\n",
            "If this inducement force her not to love,\n",
            "Send her a story of thy noble acts;\n",
            "Tell her thou madest away her uncle Clarence, and,\n",
            "Her uncle Rivers; yea, and, for her sake,\n",
            "Madest quick conveyance with her good aunt Anne.\n",
            "\n",
            "KING RICHARD III:\n",
            "Come, come, you mock me; this is not the way\n",
            "To win our daughter.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "There is no other way\n",
            "Unless thou couldst put on some other shape,\n",
            "And not be Richard that hath done all this.\n",
            "\n",
            "KING RICHARD III:\n",
            "Say that I did all this for love of her.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Nay, then indeed she cannot choose but hate thee,\n",
            "Having bought love with such a bloody spoil.\n",
            "\n",
            "KING RICHARD III:\n",
            "Look, what is done cannot be now amended:\n",
            "Men shall deal unadvisedly sometimes,\n",
            "Which after hours give leisure to repent.\n",
            "If I did take the kingdom from your sons,\n",
            "To make amends, Ill give it to your daughter.\n",
            "If I have kill'd the issue of your increase,\n",
            "To quicken your increase, I will beget\n",
            "Mine issue of your blood upon your daughter\n",
            "A grandam's name is little less in love\n",
            "Than is the doting title of a mother;\n",
            "They are as children but one step below,\n",
            "Endured of your very blood.\n",
            "Your children were vexation to your youth,\n",
            "But mine shall be a comfort to your age.\n",
            "The loss you have is but a son being king,\n",
            "And by that loss your daughter is made queen.\n",
            "I cannot make you what amends I would,\n",
            "Therefore accept such kindness as I can.\n",
            "Dorset your son, that with\n",
            "---------------\n",
            "\n",
            "And say there an hour before his last,\n",
            "That I slew him to death, he slew thy heart,\n",
            "Did I 'twas Rutland;\n",
            "But 'twas I that Rutland; which so his sake:\n",
            "Thou hadst but Edward;\n",
            "And, that ill- Rutland; which this is just,\n",
            "Didst thou revenged on him that ill-hound that doth hunt us all to death:\n",
            "That dog, to death, that makes this to death,\n",
            "To worry lambs and lap their gentle blood,\n",
            "That foul defacer of God's handiwork,\n",
            "That excellent grand tyrant of the earth,\n",
            "That reigns in galled eyes of weeping souls,\n",
            "Thy womb let loose, to chase us to our graves.\n",
            "O upright, just, just, and true-disposing God,\n",
            "How do I thank thee, that this carnal cur\n",
            "Preys on the issue of his mother's body,\n",
            "And makes her pew-fellow with others' moan!\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "O Harry's wife, triumph not in my woes!\n",
            "God witness with me, I have wept for thine.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "Bear with me; I am hungry for revenge,\n",
            "And now I cloy me with beholding it.\n",
            "Thy Edward he is dead, that stabb'd my Edward dead, to quit my Edward;\n",
            "Thy other Edward dead, to quit my Edward;\n",
            "Young York he is but boot, because both they\n",
            "Match not the high perfection of my loss:\n",
            "Thy Clarence he is dead that kill'd my Edward;\n",
            "And the beholders of this tragic play,\n",
            "The adulterate Hastings, Rivers, Vaughan, Grey,\n",
            "Untimely smother'd in their dusky graves.\n",
            "Richard yet lives, hell's black intelligencer,\n",
            "Only reserved their factor, at hand, to buy souls\n",
            "And send them thither: but at hand, at hand, at hand,\n",
            "Ensues his piteous and unpitied end:\n",
            "Earth gapes, hell burns, fiends roar, saints pray.\n",
            "To have him suddenly convey'd away.\n",
            "Cancel his bond of life, dear God, I prey,\n",
            "That I may live to say, The dog is dead!\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "O, thou didst prophesy the time would come\n",
            "That I should wish for thee to help me curse\n",
            "That bottled spider,\n",
            "---------------\n",
            "\n",
            "If you are not, and make\n",
            "My life did service indeed, for mine.\n",
            "\n",
            "KING RICHARD III:\n",
            "Your own is bankrupt so: what you brings the worst.\n",
            "\n",
            "STANLEY:\n",
            "My lord, Buckingham,\n",
            "And they indeed, to please.\n",
            "\n",
            "KING RICHARD III:\n",
            "Hoyday, Buckingham, Buckingham, as you guess?\n",
            "\n",
            "STANLEY:\n",
            "Unless for that he comes to be told.\n",
            "\n",
            "KING RICHARD III:\n",
            "Hoyday, his son George-liver'd runagate, what doth he there?\n",
            "\n",
            "STANLEY:\n",
            "Well, mighty sovereign, but by guess?\n",
            "STANLEY:\n",
            "Stirr'd up by Dorset, Buckingham, and Ely,\n",
            "He makes for England, there to claim the crown.\n",
            "\n",
            "KING RICHARD III:\n",
            "Is the chair empty? is the sword unsway'd?\n",
            "Is the king dead? the empire unpossess'd?\n",
            "What heir of York is there alive but great York's king but great York's heir?\n",
            "And who is England's king but great York's heir?\n",
            "Then, tell me, what doth he upon the sea?\n",
            "\n",
            "STANLEY:\n",
            "Unless for that, my liege, I cannot guess.\n",
            "\n",
            "KING RICHARD III:\n",
            "Unless for that he comes to be your liege,\n",
            "You cannot guess wherefore the Welshman comes.\n",
            "Thou wilt revolt, and fly to him, I fear.\n",
            "\n",
            "STANLEY:\n",
            "No, mighty liege; therefore mistrust me not.\n",
            "\n",
            "KING RICHARD III:\n",
            "Where is thy power, then, to beat him back?\n",
            "Where are thy tenants and thy followers?\n",
            "Are they not now upon the western shore.\n",
            "Safe-conducting the rebels from their ships!\n",
            "\n",
            "STANLEY:\n",
            "No, my good lord, my friends are in the north.\n",
            "\n",
            "KING RICHARD III:\n",
            "Cold friends to Richard: what do they in the north,\n",
            "When they should serve their sovereign in the west?\n",
            "\n",
            "STANLEY:\n",
            "They have not been commanded, mighty sovereign:\n",
            "Please it your majesty to give me leave,\n",
            "I'll muster up my friends, and meet your grace\n",
            "Where and what time your majesty shall please.\n",
            "\n",
            "KING RICHARD III:\n",
            "Ay, ay. thou wouldst be gone to join with Richmond:\n",
            "I will not trust you, sir.\n",
            "\n",
            "STANLEY:\n",
            "Most mighty\n",
            "---------------\n",
            "\n",
            "The slave, when we ordained festival,\n",
            "Turn back to black funeral bell;\n",
            "Our wedding cheer to melancholy bells,\n",
            "Our wedding cheer to a sad burial feast,\n",
            "Our solemn hymns to sullen dirges change,\n",
            "Our bridal flowers serve for a buried corse,\n",
            "And all things change them to the contrary.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Sir, go you in; and, madam, go with him;\n",
            "And go, Sir Paris; every one prepare\n",
            "To follow this fair corse unto her grave:\n",
            "The heavens do lour upon you for some ill;\n",
            "Move them no more by crossing their high will.\n",
            "\n",
            "First Musician:\n",
            "Faith, we may put up our pipes, and be gone.\n",
            "\n",
            "Nurse:\n",
            "Honest goodfellows, ah, put up;\n",
            "For, well you know, this is a pitiful case.\n",
            "First Musician:\n",
            "Ay, by my troth, the case may be amended.\n",
            "\n",
            "PETER:\n",
            "Musicians, O, musicians, 'Heart's ease, Heart's\n",
            "ease:' O, an you will have me live, play 'Heart's ease.'\n",
            "\n",
            "First Musician:\n",
            "Why 'Heart's ease?'\n",
            "\n",
            "PETER:\n",
            "O, musicians, because my heart itself plays 'My\n",
            "heart is full of woe:' O, play me some merry dump,\n",
            "to comfort me.\n",
            "\n",
            "First Musician:\n",
            "Not a dump we; 'tis no time to play now?\n",
            "\n",
            "PETER:\n",
            "You will not, then?\n",
            "\n",
            "First Musician:\n",
            "No.\n",
            "\n",
            "PETER:\n",
            "I will then give it you soundly.\n",
            "\n",
            "First Musician:\n",
            "What will you give us?\n",
            "\n",
            "PETER:\n",
            "No money, on my faith, but the gleek;\n",
            "I will give you the minstrel.\n",
            "\n",
            "First Musician:\n",
            "Then I will give you the serving-creature.\n",
            "\n",
            "PETER:\n",
            "Then will I lay the serving-creature's dagger on\n",
            "your pate. I will carry no crotchets: I'll re you,\n",
            "I'll fa you; do you note me?\n",
            "\n",
            "First Musician:\n",
            "An you re us and fa us, you note us.\n",
            "\n",
            "Second Musician:\n",
            "Pray you, put up your dagger, and put out your wit.\n",
            "\n",
            "PETER:\n",
            "Then have at you with my wit\n",
            "---------------\n",
            "\n",
            "I could make them that, they say so high,\n",
            "As those should have cause to bear them take hold their hats.\n",
            "But, my lord, let us away.\n",
            "\n",
            "HASTINGS:\n",
            "Go on before; I'll talk.\n",
            "How now, sirrah! how goes the world with thee?\n",
            "\n",
            "Pursuivant:\n",
            "The better that your lordship please to ask.\n",
            "\n",
            "HASTINGS:\n",
            "I tell thee, man, 'tis better with me now\n",
            "Than when I met thee last where now we meet:\n",
            "Then was I going prisoner to the Tower,\n",
            "By the suggestion of the queen's allies;\n",
            "But now, I tell thee--keep it to thyself--\n",
            "This day those enemies are put to death,\n",
            "And I in better state than e'er I was.\n",
            "\n",
            "Pursuivant:\n",
            "God hold it, to your honour's good content!\n",
            "\n",
            "HASTINGS:\n",
            "Gramercy, fellow: there, drink that for me.\n",
            "\n",
            "Pursuivant:\n",
            "God save your lordship!\n",
            "\n",
            "Priest:\n",
            "Well met, my lord; I am glad to see your honour.\n",
            "HASTINGS:\n",
            "I thank thee, good Sir John, with all my heart.\n",
            "I am in your last exercise;\n",
            "Come the next Sabbath, and I will content you.\n",
            "BUCKINGHAM:\n",
            "What, talking with a priest, lord chamberlain?\n",
            "Your friends at Pomfret, they do need the priest;\n",
            "Your honour hath no shriving work in hand.\n",
            "\n",
            "HASTINGS:\n",
            "Good faith, and when I met this holy man,\n",
            "Those men you talk of came into my mind.\n",
            "What, go you toward the Tower?\n",
            "\n",
            "BUCKINGHAM:\n",
            "I do, my lord; but long I shall not stay\n",
            "I shall return before your lordship thence.\n",
            "\n",
            "HASTINGS:\n",
            "'Tis like enough, for I stay dinner there.\n",
            "\n",
            "BUCKINGHAM:\n",
            "HASTINGS:\n",
            "I'll wait upon your lordship.\n",
            "\n",
            "RATCLIFF:\n",
            "Come, bring forth the prisoners.\n",
            "\n",
            "RIVERS:\n",
            "Sir Richard Ratcliff, let me tell thee this:\n",
            "To-day shalt thou behold a subject die\n",
            "For truth, for duty, and for loyalty.\n",
            "\n",
            "GREY:\n",
            "God keep the prince from all the pack of you!\n",
            "A knot you are of damned blood-suckers!\n",
            "---------------\n",
            "\n",
            "LEONTES:\n",
            "Hermione was it, not so.\n",
            "\n",
            "PAULINA:\n",
            "So much more, my good,\n",
            "Not she might have thus she lived now.\n",
            "\n",
            "PAULINA:\n",
            "So much the more such as it is\n",
            "O, my good comfort, as it is\n",
            "In such life, as it is\n",
            "But one of himself, as you.\n",
            "\n",
            "LEONTES:\n",
            "If I am none, I, my lord and graced\n",
            "You knew of those\n",
            "Would have him wed again.\n",
            "\n",
            "PAULINA:\n",
            "As\n",
            "If I had thought no more benefit and graced\n",
            "You break a perpetual all, nor the remembrance\n",
            "Of his most sovereign name; consider little\n",
            "What dangers, by his highness' fail of issue,\n",
            "May drop upon his kingdom and devour\n",
            "Incertain lookers on. What were more holy\n",
            "Than to rejoice the former queen is well?\n",
            "What holier than, for royalty's repair,\n",
            "For present comfort and for future good,\n",
            "To bless the bed of majesty again\n",
            "With a sweet fellow to't?\n",
            "\n",
            "PAULINA:\n",
            "There is none worthy,\n",
            "Respecting her that's gone. Besides, the gods\n",
            "Will have fulfill'd their secret purposes;\n",
            "For has not the divine Apollo said,\n",
            "Is't not the tenor of his oracle,\n",
            "That King Leontes shall not have an heir\n",
            "Till his lost child be found? which that it shall,\n",
            "Is all as monstrous to our human reason\n",
            "As my Antigonus to break his grave\n",
            "As my Antigonus to break his grave\n",
            "Did perish with the infant. 'Tis your counsel\n",
            "My lord should to the heavens be contrary,\n",
            "Oppose against their wills.\n",
            "Care not for issue;\n",
            "The crown will find an heir: great Alexander\n",
            "Left his to the worthiest; so his successor\n",
            "Was like to be the best.\n",
            "\n",
            "LEONTES:\n",
            "Good Paulina,\n",
            "Who hast the memory of Hermione,\n",
            "I know, in honour, O, that ever I\n",
            "Had squared me to thy counsel! then, even now,\n",
            "I might have look'd upon my queen's full eyes,\n",
            "Have taken treasure from her lips--\n",
            "\n",
            "PAULINA:\n",
            "And left them\n",
            "More rich for what they yielded.\n",
            "\n",
            "LEONTES:\n",
            "Thou speak'st truth.\n",
            "---------------\n",
            "\n",
            "Not to be so much as a man; but it is as\n",
            "y. But, Camillo, are to be his departure, are to be, as you\n",
            "ne of the king's, are ignorant:\n",
            "May not as you not honest\n",
            "Your queen, as you are well enjoy,\n",
            "Or to you and so,\n",
            "Would fetch me, as I think honourable:\n",
            "I have loved you, and so craftily;\n",
            "But graciously to know I am no better.\n",
            "\n",
            "LEONTES:\n",
            "By this we speak I am not\n",
            "For this seems so, and in that,\n",
            "But graciously to know I am no better.\n",
            "\n",
            "ANTIGONUS:\n",
            "A gross is not, I am no other, I'll speak I am no better.\n",
            "\n",
            "LEONTES:\n",
            "Force:\n",
            "I dare not:\n",
            "But yet I, you that, and I shall not\n",
            "As this which I'll no better\n",
            "Than you.\n",
            "\n",
            "First Lord:\n",
            "I' the oracle, my queen and more it must not\n",
            "PAULINA:\n",
            "The keeper of the prison,\n",
            "What's the prison is gone?\n",
            "\n",
            "LEONTES:\n",
            "Apollo's angry;\n",
            "Do strike at my queen: look down\n",
            "Do strike at my queen: look upon\n",
            "Ind see the hands, and there\n",
            "Will answerPer'd upon her\n",
            "My lord Paulina.\n",
            "\n",
            "PAULINA:\n",
            "I did not stumble.\n",
            "\n",
            "LEONTES:\n",
            "I told her so.\n",
            "\n",
            "PAULINA:\n",
            "It is; and is for she should be.\n",
            "\n",
            "PAULINA:\n",
            "Nor I, sir,\n",
            "That creep like to know't\n",
            "From your own accord I'll off;\n",
            "But first I'll do my errand. The good queen,\n",
            "For she is good, hath brought you forth a daughter;\n",
            "Here 'tis; commends it to your blessing.\n",
            "\n",
            "LEONTES:\n",
            "Out!\n",
            "A mankind witch! Hence with her, out o' door:\n",
            "A most intelligencing bawd!\n",
            "\n",
            "PAULINA:\n",
            "Not so:\n",
            "I am as ignorant in that as you\n",
            "In so entitling me, and no less honest\n",
            "Than you are mad; which is enough, I'll warrant,\n",
            "As this world goes, to pass for honest.\n",
            "\n",
            "LEONTES:\n",
            "Traitors!\n",
            "Will you not push her out? Give\n",
            "---------------\n"
          ]
        }
      ]
    }
  ]
}